\lab{Python}{Scrapy}{Scrapping}
\objective{Teach students how to scrape websites}

\section*{Scrapying Data}
Scrapping data from a website is a way to extract structured or unstructured data from the HTML of a page.

Most websites have APIs that direct people how to take data from the webpage. Often you use the API. You use a scrapying tool if you want to extract data from a website that does not have an API or the data you want is not a part of the API. (Like extracting e-mail addresses of users)

There is a nice python package called scrapy that is an application framework for scrapying. . The offical documentation can be found at \url{http://doc.scrapy.org/en/latest/}. 

Another package is BeautifulSoup the Documentation can be found at \url{http://www.crummy.com/software/BeautifulSoup/bs4/doc/}

\begin{info}
You will making and editing python files in this lab, so you will not be using ipython.
\end{info}

\begin{warn}
This Lab was written using version 0.22 of scrapy. If you are using a different version, the code will not be exactly the same.
\end{warn}

\section*{Installation}

To learn how to install Scrapy go to the website \url{http://doc.scrapy.org/en/latest/intro/install.html}. This also contains helpful tutorials and documentation for Scrapy.

\section*{Setting up}

\begin{comment}
\begin{problem}
Work thourgh the tutorial from the url above. You should turn in items.json as specified in the tutorial.
\end{problem}


When you run
\begin{lstlisting}
scrapy crawl dmoz -o items.json -t json
\end{lstlisting}
which generates a file \li{items.json} where all the scraped items are serialized in JSON. You can also store them in csv or xml by changing all the json to csv or xml.
For example
\begin{lstlisting}
scrapy crawl dmoz -o items.csv -t csv
\end{lstlisting}
generates a file \li{items.csv} where all the scraped items are stored in a csv file.
\end{comment}


To begin a project go to the directory where you want to store the code and run
\begin{lstlisting}
scrapy startproject <project name>
\end{lstlisting}
This will create several files. \li{scrapy.cfg} is a configuration file. It will create folder called \li{<project name>} , that contains files \li{items.py}, \li{pipelines.py}, \li{settings.py} and a directory called \li{spiders}. We will use all this files.

A scrapy project is broken into parts. The \li{spiders} directorycontain the "spiders" that will parse and follow link on the web, depending on what you dictate them. In the \li{items.py} file ( which can be broken up into multiple files) contains definitions of the data items that your spiders will produce. These items are then passed to functions in \li{pipelines.py} which carry out what you want to be done with the items (i.e. save them in a SQL database or csv file, further process them).

\section*{Items}
Items are like python dictionaries with some addtional functionality. Go to the \li{items.py} found in the \li{<project name>} directory
and put in it the following code.
\begin{lstlisting}
from scrapy.item import Item, Field

class ExampleItem(Item):
    title = Field()
    link = Field()
    desc = Field()
\end{lstlisting}

This defines our items with the 3 fields \li{title},\li{link}, and \li{desc}. Notice that our item class inherits from the Scrapy class \li{Item} and our fields are instances of \li{Field}. These definitions will give us extra functionality from Scrapy.

\section*{Spiders}
Spiders are classes used to scrape data from a group of websites. You will define classes that define the initial list of URLs to download and begin on, how to follow the links, and how to parse the contents of the pages into your items objects. We will use two spiders. \li{Spider} and \li{CrawlSpider}. A \li{Spider} is used for a static number of links while a CrawlSpider can be used for crawling mulitple links, searching each for more links to follow and crawl.

We will start with a \li{Spider}. Go in your spider directory and create a new file (the name of the file does not matter). Inherit from \li{Spider} as shown below. A \li{Spider} has three important parts: \li{name} which is a unique identifier you will call it from the commandline later (note that it must be unique), \li{allowed_domains} which specifies the websites that the spider is allowed to crawl (more important for \li{CrawlSpider}, \li{start_urls} which is a list of URLS from which the spider will begin to crawl from and the method \li{parse} which will parse the \li{response} oject generated by Scapy when it queries each link.

the following is an example.
\begin{lstlisting}
from scrapy.spider import Spider

class ExampleSpider(Spider):
    name = "myExampleSpider"
    allowed_domains = ["example.org"]
    start_urls = [
        "http://www.example.org/Computers/",
        "http://www.example.org/books/"
    ]

    def parse(self, response):
        sel = Selector(response)
        hits = sel.xpath('//tr/td')

        items = [] #initialize as an empty list
        for hit in hits:
            item = ExampleItem()
			#fields are accessed like a python dictionary
            item['title'] = hit.xpath('a/text()').extract()
            item['link'] = hit.xpath('a/@href').extract()
            item['desc'] = hit.xpath('text()').extract()
            items.append(item)#add each item as they are parsed

		filename = response.url.split("/")[-2]
        open(filename, 'wb').write(response.body)
        return items

\end{lstlisting}

The two penultimate lines save each page in a file so that we can examine the actual html that Scrapy sees.
Now we will explain the actual parsing the the \li{parse} method.

\section*{Parsing}
Scrapy uses classes called selectors to extract data from web pages.
There are many methods in \li{Selector}, we will focus on three.

The \li{xpath(expression)} method returns a list of selectors, each of them representing html nodes by the expression given as the arguement. This involves knowing a little HTML. HTML has elements whose name is enclosed in <>. In the code above \li{xpath('//tr/td')} selects  all the \li{td}'s within a \li{tr} node ( the '//' in our expression specifies all matches in the document, even outside of the current scope). For every \li{Selector} in the resulting list we apply \li{expression('a/text()')} to obtain the text inside of every \li{a} (hyperlink tag) and \li{xpath('a/@href')} to get the data from the \li{href} atribute from the \li{a} tag (in html the link that the hyperlink points to). 

The \li{extract()} methed simply returns the string of the data selected by the selector.

For example, if we were to apply our parser to the following piece of html

\begin{lstlisting}[language=HTML]
<div class="content" style="">
	<table>
		<tr>
			<th> not a hit </th>
			<th> not a hit </th>
		</tr>
		<tr>
			<th
			<td id="first" class="link">
				This is the start of the desc text
				<a href="http://www.example.org/this/is/our/link.html"> This is our link text</a>
				This is the end of the desc text
			</td>
		
			<th id="second">
				This is another hit but without an -a- tag
			</td>
		</tr>
		<tr>
			<td id="third" class="link">
				<a href="http://www.example.org/another/link.html"> This is our link text</a>
			</td>
			<td id="fourth">
				one last hit
			</td>
	</table>
</div>
\end{lstlisting}
\li{hits} would be populated with four Selectors ( the <div> tag is completely ignored), each will produce an item in our \li{items} list. The first will have \li{item.title="This is our link text"}, \li{item.link="http://this/is/our/link.html", and \li{item.desc} would be the entire text between \li{<th>} and \li{</th>}. The second item will have \li{item.title} and \li{item.desc} empty.

\li{xpath} can also select nodes with a specific values for it's atributes. For example \li{/th[@class="link"]'} will select all \li{th} tags with it's \li{class} attribute equal to \li{"link"} starting at the root node ( because of the single '/', which refers to the \li{div} tag in this case). \li{/th/a[1]} will select the first \li{a} tag within a \li{th}, \li{/th/a[last()]} will select the last and \li{/th/a[last()-1]} the second to last.

We will also use \li{re(regular expression)}, which returns a list of all the strings extracted by applying the regular espression given as the argument.

To run this spider you would enter into the commandline
\begin{lstlisting}
scrapy crawl myExampleSpider
\end{lstlisting}

Scrapy also provides an interactive terminal for expirementing.
\begin{lstlisting}
scrapy shell <URL of website>
\end{lstlisting}

Try \li{scrapy shell http://www.dmoz.org/} and \li{sel.xpath('//title')}. You will get feedback as if you were using the the \li{sel.xpath()} in your code. You can try other methods to see what they do.

\begin{problem}
Create a new project called \li{dmoz} and create a spider to scrape from the domain \li{"dmoz.org"} the two links \li{http://www.dmoz.org/Computers/Programming/Languages/Python/Books/} and \li{http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/}. Use the same item class ( but you should rename it appropriately) and modify the \li{parse} function of the spider to produce items for each \li{li} in a \li{ul} tag. Store the text of each \li{a} tag in \li{item.title} and the actual link in \li{link}. Store the entire text of the \li{li} tag in \li{item.desc}. You can leave in the two lines that store the html source when you first run your script, but you should not include them in your final solution. Store the resulting items in a \li{.csv} file by running in the commandline \li{scrapy crawl yourSpider -o items.csv -t csv}.
\end{problem}

\section*{Crawling the Web}
This follows the section \url{http://doc.scrapy.org/en/latest/topics/spiders.html} subsection CrawlSpider.

The spider you wrote from the tutorial will only scrape one page. In order to scrape more pages you will need to inherit from \li{CrawlSpider}. The main difference is you set a list of rules for how you spider clicks on links and you do not overwrite the parse() method.

\begin{warn}
When using a \li{CrawlSpider} do not overwrite the \li{parse} method.
\end{warn}

For example, to scrape a website for sports statistics, this code would start of the mainpage and click are the links as specified in the rules. So below I made a\li{rule} list and for each rule I cread a \li{Rule} object. The first varible a takes in is a link extractor. the most common one is the li{SgmlLinkExtractor}. Two of its many attributes it has is \li{allow} and \li{deny}. \li{allow} takes in a list of strings and will click on those links has those words in the url. \li{deny} also takes in a list of strings and will not click on links with those words in the url. \li{deny} has higher precedence than \li{allow}. For a \li{Rule} object you set \li{callback} as a string of the method that parses the pages that you click on with the link extractor.  And than you set \li{follow=True} which means that it will click on those links.

The following code will go to the espn nba statistics website and click on the links that have \li{'statistics/team/_/stat'} in the name but not \li{'sort'} and scrape those webpages.

To do change
\begin{lstlisting}
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector

from sportscrape.items import SportscrapeItem

class SportsSpider(CrawlSpider):
   name = "espn"
   allowed_domains = ["mysports.com"]
   start_urls = [
       "http://espn.go.com/nba/statistics"
   ]
   rules = (
        Rule(SgmlLinkExtractor(allow=('statistics/team/_/stat',),deny=('sort',)),callback='parse_page',follow=True),
    )

   def parse_page(self, response):
       sel = Selector(response)
       sites = sel.xpath('//td')
       items = []
       for site in sites:
           item = SportscrapeItem()
           item['numbers'] = site.xpath('text()').extract()
           item['names'] = site.xpath('a/text()').extract()
           items.append(item)
       return items
\end{lstlisting}

\section*{Pipelining}

The documentation can be found \url{http://doc.scrapy.org/en/latest/topics/item-pipeline.html}
Now you have a spider that crawls the web and dumps all the data into csv or json file. For some applications that will be enough. 

In addition you can cleanse the data or put it into a database via pipelining. In your directory \li{<project name>} you will have a file called \li{pipelines.py}. In this file you can create a class that has three methods \li{process_item(item, spider)}, \li{open_spider(spider)} and \li{close_spider(spider)}. The methods \li{open_spider} and \li{closed_spider} all called when the project is started and ended respectively. The \li{process_item} is called when the \li{parse} method of the spider returns. 

\subsection*{Cleansing}

The following code drops an item if it does not have anything in the data part of the object and returns it otherwise.

\begin{lstlisting}
from scrapy.exceptions import DropItem

class DataPipeline(object):

    def process_item(self, item, spider):
        if item['data']:
            return item
        else:
            raise DropItem("Missing data in %s" % item)
\end{lstlisting}

\subsection*{Adding to a database}
The following code puts the items where both its attributes are not null into a database. 
\begin{lstlisting}
import sqlite3 as sql
class SportscrapePipeline(object):
	
	def open_spider(self,spider):
		self.db = sql.connect("test1")
		self.cur = self.db.cursor()
		self.cur.execute('DROP TABLE IF EXISTS vals')
		self.cur.execute('CREATE TABLE vals (name TEXT, numbers TEXT);')
		self.statement = "INSERT INTO vals VALUES(?, ?);"
		return

	def close_spider(self, spider):
		self.db.commit() #save changes made in the transaction
		self.db.close()
		return

	def process_item(self, item, spider):
		#data=(item['names'][0],item['numbers'][0])
		if item['names'] and item['numbers']:
			data=(item['names'][0],item['numbers'][0])
			self.cur.execute(self.statement, data)
		elif item['names'] and  not item['numbers']:
			data=(item['names'][0],"null")
			self.cur.execute(self.statement, data)
		elif not item['names'] and item['numbers']:
			data=("null",item['numbers'][0])
			self.cur.execute(self.statement, data)
\end{lstlisting}

\subsection*{Activating your Pipeline}
Once you have written your pipeline code to activate it you go to your \li{settings.py} file and add the line
\begin{lstlisting}
ITEM_PIPELINES = '<directory>.pipelines.<name of class>': 300
\end{lstlisting}
where \li{<directory>} is the directory, \li{pipelines} stands for \li{pipelines.py} file, and \li{<name of class>} is what you named your pipeline class. In the example above it would be \li{SportscrapePipeline}. The \li{300} is a precedence operator that takes effect if you have multiple pipelines.

\begin{problem}
Write a webscraper that starts \url{http://math.byu.edu/peopleresearch/faculty/} clicks on links  in order to store each the professors name, email, office number, phone number (not fax number), and email address. If data is missing on the webpage put null. Store it in a csv file. Turn in the csv file. Call it \li{ProfessorData.csv}.

Hint: Use regular expressions.
\end{problem}

\begin{problem}
Create a pipeline to store the data from the last problem in a database. Turn in the database file. Call it \li{ProfessorData}.
\end{problem}

For this lab you turn in \li{items.json}, \li{ProfessorData.csv}, \li{ProfessorData} and your \li{spider.py} file from the second and third problem into your google drive.
