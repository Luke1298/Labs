\lab{Python}{Scrapy}{Standard Library}
\objective{Teach students how to scrape websites}

\section*{Scrapying Data}
Scrapping data from a website is a way to extract structured or unstructured data from the HTML of a page.

Most websites have APIs that direct people how to take data from the webpage. Often you use the API. You use a scrapying tool if you want to extract data from a website that does not have an API or the data you want is not a part of the API. (Like extracting e-mail addresses of users)

There is a nice python package called scrapy that is an application framework for scrapying. This lan follows that you using version 0.16. THe offical documentation can be found at \url{http://doc.scrapy.org/en/latest/}. 

Another package is BeautifulSoup the Documentation can be found at \url{http://www.crummy.com/software/BeautifulSoup/bs4/doc/}

\begin{info}
You will making and editing python files in this lab, so you will not be using ipython.
\end{info}

\section*{Setting up}

To begin a project go to the directory where you want to store the code and run
\begin{lstlisting}
scrapy startproject <project name>
\end{lstlisting}
This will create several files. \li{scrapy.cfg} is a configuration file. It will create folder called \li{<project name>} , that contains files \li{items.py}, \li{pipelines.py}, \li{settings.py} and a directory called \li{spiders}. We will use all this files.

\section*{Items}
Items are like python dictionaries with some addtional functionality. Go to the items.py found in the  \li{<project name>} directory. items are labels for the data that you be storing.

\begin{lstlisting}
from scrapy.item import Item, Field

class <project name>Item(Item):
    <item1> = Field()
    <item2> = Field()
    <item3> = Field()
\end{lstlisting}

\section*{Spiders}
Spiders are classes used to scrape data from a group of websites. You will define calsses that define the initail list of URLs to download, how to follow the links, and how to parse the contents of the pages into your items objects. There are two main spiders. \li{BaseSpider} and \li{CrawlSpider}. A BaseSpider is used for one website and a CrawlSpider used for crawling mulitple websites.

We will start with a BaseSpider. Go in your spider directory and create a new file (the name of the file does not matter). Subcalss BaseSpider as shown below. A BaseSpider has three things. A name that is a unique identifier, start_urls which is a list of URLS from which the spider will begin to crawl from and parse(), a method which will be called with the Response object at the beginning of each url.

the following is an example.
\begin{lstlisting}
from scrapy.spider import BaseSpider

class DmozSpider(BaseSpider):
    name = "example"
    allowed_domains = ["example.org"]
    start_urls = [
        "http://www.example.org/Computers/",
        "http://www.example.org/books/"
    ]

    def parse(self, response):
        hxs = HtmlXPathSelector(response)
        sites = hxs.select('//td')
        for site in sites:
            title = site.select('a/text()').extract()
            link = site.select('a/@href').extract()
            desc = site.select('text()').extract()
            print title, link, desc
\end{lstlisting}

Next we are going to explain what is in the parse method.

\section*{Parsing}
Scrapy uses a mechanism called selectors to extract data from web pages.
They have thhree methods.

the \li{select(path)} method returns a list of selectors, each of them representing nodes by the expression given as the arguement.This involves knowing a little HTML. HTML has elements whose name is enclosed in <>. So in the code above \li{select('//td')} selects  all the \li{<td>} elements. \li{select('//a/text()')} selects the text inside the \li{<a>} elements. the \li{//div[@class="mine"]} selects all the \li{<div>} elements with the attirbute \li{class="mine"}.  

The \li{extract()} methoed return a string with the data selected by the selector.

The \li{re(string)} returns a list of strings extracted by applying the regular espression given as the argument.

You can test this in a shell by typing in
\begin{lstlisting}
scrapy shell <URL of website>
\end{lstlisting}

try \li{scrapy shell http://www.dmoz.org/} and \li{hxs.select('//title')}. You will get feedback as if you were using the the \li{hxs.select()} in your code. You can try other methods to see what they do.

\section*{Storing the data}
To store the data we use our items. 

Say we defined our items like so
\begin{lstlisting}
from scrapy.item import Item, Field

from scrapy.item import Item, Field

class SportscrapeItem(Item):
    names = Field()
    numbers = Field()
\end{lstlisting}

And we definded our spider like so.
\begin{lstlisting}
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector

from tutorial.items import DmozItem

class DmozSpider(BaseSpider):
   name = "dmoz"
   allowed_domains = ["dmoz.org"]
   start_urls = [
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
       "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
   ]

   def parse(self, response):
       hxs = HtmlXPathSelector(response)
       sites = hxs.select('//ul/li')
       items = []
       for site in sites:
           item = DmozItem()
           item['title'] = site.select('a/text()').extract()
           item['link'] = site.select('a/@href').extract()
           item['desc'] = site.select('text()').extract()
           items.append(item)
       return items
\end{lstlisting}

Then I would go to the top directory and run
\begin{lstlisting}
scrapy crawl dmoz -o items.json -t json
\end{lstlisting}
which will generate a file \li{items.json} where all the scraped items are serialized in JSON. You can also store them in csv or xml by changing all the json to csv or xml.

\section*{Crawling the Web}
The spider above will only scrape one page. In order to scrape more pages you will need to inherit from \li{CrawlSpider}. The main difference is you set a list of rules for how you spider clicks on links and you do not over write the parse() method.

\begin{warn}
When using a \li{CrawlSpider} do not overwrite the parse method.
\end{warn}

For example if I were scrapying the ESPN website for basketball statistics, this code would start of the mainpage and click are the links as specified in the rules. So below I made a\li{rule} list and for each rule I cread a \li{Rule} object. The first varible a takes in is a link extractor. the most common one is the li{SgmlLinkExtractor}. Two of its many attributes it has is \li{allow} and \li{deny}. \li{allow} takes in a list of strings and will click on those links has those words in the url. \li{deny} also takes in a list of strings and will not click on links with those words in the url. \li{deny} has higher precedence than \li{allow}. For a \li{Rule} object you set \li{callback} as a string of the method that parses the pages that you click on with the link extractor.  And than you set |li{follow=True} which means that it will click on those links.

The following code will go to the espn nba statistics website and click on the links that have \li{'statistics/team/_/stat'} in the name but not \li{'sort'} and scrape those webpages.

\begin{lstlisting}
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector

from sportscrape.items import SportscrapeItem

class DmozSpider(CrawlSpider):
   name = "espn"
   allowed_domains = ["espn.go.com"]
   start_urls = [
       "http://espn.go.com/nba/statistics"
   ]
   rules = (
        Rule(SgmlLinkExtractor(allow=('statistics/team/_/stat',),deny=('sort',)),callback='parse_page',follow=True),
    )

   def parse_page(self, response):
       hxs = HtmlXPathSelector(response)
       sites = hxs.select('//td')
       items = []
       for site in sites:
           item = SportscrapeItem()
           item['numbers'] = site.select('text()').extract()
           item['names'] = site.select('a/text()').extract()
           items.append(item)
       return items
\end{lstlisting}

\section*{Database}
