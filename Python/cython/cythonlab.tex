\lab{Computing with Cython}{Cython}
\label{lab:Cython}
\objective{Use Cython to avoid the overhead of some parts of the Python language.}

Python is reasonably fast for computing the solutions to small problems.
However, for large computations Python quickly becomes unable to solve anything in a reasonable amount of time.
Why then is Python used if it's so slow?  One important answer to this question is programmer productivity.
The amount of time required to write code is dramatically less than the time required by low-level programming languages.

Low-level languages like C are renowned for their speed.
These languages are compiled to machine code that is better adapted on a computer's processor.
When a C program adds two numbers together, we have to be very specific about what two numbers are added together.
The computer can understand basic data types such as integers, floats, and arrays.
Python represents each of these data types using its own objects.
These are objects are generally fast and we don't notice much the incurred overhead.
Over large calculations, the computational cost of these objects is compounded and can slow things down tremendously.
In these cases, we need to be able to access the machine-level data types for integers, floats, and strings for maximum efficiency.
A common way of doing this is to write slower portions of the code in a lower-level, compiled language.
This is usually accomplished by writing a C-extension for Python.
A C-extension is a module that can be used like a Python module, but is written in C.
Python is written in C, and it has well-defined ways to perform operations on Python objects from C.
This enables performance critical code to be executed as fast as possible in a Python environment.
Writing these C-extensions can often be difficult and tedious work.

Much of the time, the operations that are performed in these C extensions are direct analogs of simple operations from either Python or C, but the interface for working with both languages simultaneously often complicates what would otherwise be a series of simple operations.
Cython was created to automate the creation of these C extension modules.
As a language, \textbf{it is essentially just Python with some extra type declarations.}
Using Cython, C-extensions can be written very quickly and give all the benefits of C speed.
Cython also makes it easier to avoid bugs (like memory leaks) that often arise when writing C extensions.

Cython is like Python with added syntax that enables you to call C functions and declare C types on Python variables.
The additional type declarations of Cython allow it to generate highly optimized C code that will compile with all major C/C++ compilers.
Beginning users often use it to speed up loops and array access times.
It is often used to create Python interfaces for C and C++ libraries and to write well-optimized Python libraries.
Since Cython outputs C code, you can easily call any C function from any point in your Python program.

Cython is used extensively in many packages.
SciPy uses Cython extensively in many of its packages for both improved speed and creating interfaces to packages that are written in C or C++.
It also uses a tool called f2py to interface with Fortran (another fast programming language).
Most core NumPy operations are written exclusively in C, but some of the components in its \li{random} module are written in Cython.
Several other packages like pyfftw, pandas, scikit-learn, scikit-image, and astropy use Cython extensively.
The package Kivy, that is used to write Android and iOS apps in Python, relies heavily on Cython.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{compilation.pdf}
\caption{A diagram of how Cython compilation and calls to Cython functions work.
The path from using a Cython function in a Python file to the actual evaluation is shown in red.}
\label{cython:compilation}
\end{figure}

\begin{warn}
Don't expect Cython to speed up Python code automatically.
If you run unmodified Python code through the Cython compiler, the result may be somewhat faster, but it may not.
The benefits from Cython come when you change some portions of your code (like loops and operations that are performed inside loops) so that they are performed in C and no longer need to use generic Python objects.
\end{warn}

\begin{warn}
Poorly written code will not perform well, regardless of the language in which it is written.
It is often easier and more effective to focus on writing good code in Python than it is to try to interface with a different ``faster" language.
If you need to speed up your code, first profile it to see which parts are slow (IPython's magic function \li{\%prun} can be used for this), then focus on removing unneeded operations from the slow portion of your code.
Effective use of array operations in NumPy can often make code simpler and faster than it would have been if you had tried to write all the operations yourself in C, Fortran, or any other language.
Cython should be used when other attempts at optimization and vectorization are insufficient or when you need to use functionality from a C or C++ library that is not available in Python.
\end{warn}

Cython has a more complex compilation process than Python and is not usually run interactively.
Cython code is usually written in a \texttt{.pyx} file.
Such a file is then compiled to C, and then the C is compiled to machine code.
The resulting file is a Python extension that can be imported from the Python interpreter.
Many of the same built in features, data types, and functions from Python work in Cython, but keep in mind that too many calls to Python based functions may slow down a Cython program.
Figure \ref{cython:compilation} shows how a Cython file is compiled and how a function call to a Cython module works.
Regardless of what method you use to compile a Cython file, this is more or less how it works.

There are a variety of ways to import Cython functions and classes into Python.
The standard way is to make a Python file that builds the C-extension module in a given directory.
If your module needs some special compilation configuration, you will need to do it this way.
Instructions on how to do this can be found at \url{http://docs.cython.org/src/reference/compilation.html}.
The Sage and IPython notebooks have seamless integrations with Cython via cell magic extensions.
In Sage, the cell magic is \li{\%cython}.
In the IPython notebook, you must first load the extension with \li{\%load_ext cythonmagic} (or \li{\%load_ext Cython} if you are using more recent versions of IPython and Cython).
Then the first line of any cell containing Cython code should be \li{\%\%cython}.
More instructions on how to use IPython's Cython magic and other related magic functions can be found in the IPython documentation.
Cython also comes with a Python module, \li{pyximport}, that will automatically generate, compile, and import most Cython files.
\begin{lstlisting}
import pyximport
pyximport.install()
\end{lstlisting}

\section*{Type Declarations}
One of the easiest ways improve performance in Cython is to declare types on variables.
Python does not enforce strict type checking.
Any variable can be any type and that type can change at any time.
To allow this behavior, an extra interface layer is needed.
Cython allows us to declare a variable of a certain type.
These typed variables are not Python objects, but native machine-types.

In Python, you can iterate over a list, or use a generator object.
In C, for-loops are managed by indexing an integer and checking to see if it is within a range of allowed values.
This is a good example of how using Python objects for everything can slow things down.
In Python a for-loop from 0 to 1000000 would look something like this:
\begin{lstlisting}
for i in range(1000000):
    pass
\end{lstlisting}
This for-loop creates a Python list of 1000000 objects and then iterates through it.
This can  be very slow.
It also uses a sizeable chunk of memory for no particular reason.
We can improve on this by using the \li{xrange} function as follows:
\begin{lstlisting}
for i in xrange(1000000):
    pass
\end{lstlisting}

Rather than making a list, \li{xrange} makes a generator object that returns the values as needed.
For large lists, this can give be a few times faster.
Cython will run this loop roughly the same way that Python would.
If, however, we pre-define the type of the loop variable, Cython will change the loop so that it is run in C.
A \li{cdef} statement can be used to define \li{i} as a C integer type with the syntax \li{cdef <type> <name>}.
The empty for-loops above may be written in Cython as follows:
\begin{lstlisting}
cdef int i
for i in range(1000000):
    pass
\end{lstlisting}

Because Cython uses the extra type information, it can translate the loop into C, resulting in a speed-up of a factor of 50 over Python.
Cython achieves this speed-up by removing the cost of working with Python integer objects.
Similarly, when summing a large array of double precision floating point values, declaring the variable type will speed up the computation.
It is much easier for a computer to perform arithmetic operations without having to infer what datatypes are being used when each operation is performed.
Generally, whenever a large number of computations involve the same data type, adding type declarations should speed things up considerably.

Table \ref{table:cython_types} shows the different numeric types that are available in Cython.
Cython uses the same type naming conventions as C and C++.
On most normal modern systems, the sizes listed for integers will apply, though some are still dependent on the operating system.

\begin{table}
\begin{center}
\begin{tabular}{|p{2.8cm}|p{2.5cm}|p{6cm}|}
\hline
Cython Type & NumPy Type& Description \\
\hline
float & float32 & 32 bit floating point number \\
double & float64 & 64 bit floating point number \\
float complex & complex64 & 64 bit floating point complex number \\
double complex & complex128 & 128 bit floating point complex number \\
char & int8 & 8 bit signed integer \\
unsigned char & uint8 & 8 bit unsigned integer \\
short & int16 & 16 bit signed integer \\
unsigned short & uint16 & 16 bit unsigned integer \\
int & int32 & 32 bit signed integer \\
unsigned int & uint32 & 32 bit unsigned integer \\
long & int32 or int64 & 32 or 64 bit signed integer (size depends on platform) \\
unsigned long & uint32 or uint64 & 32 or 64 bit unsigned integer (size depends on platform ) \\
long long & int64 & 64 bit signed integer \\
unsigned long long & uint64 & 64 bit unsigned integer \\
\hline
\end{tabular}
\end{center}
\caption{Numeric types available in Cython.}
\label{table:cython_types}
\end{table}

\begin{warn}
Integer types in C are different from Python's integer type.
Python's integer type works with arbitrarily large integers.
C integers can overflow.
When you declare the type of a variable in Cython to be some sort of integer, it is declared as a C integer type and can overflow.
\end{warn}

\section*{Optimized Array Access}
It is often useful to iterate over large arrays very quickly.
The \li{__getitem__} method (equivalent to array access using \li{[ ]}) of NumPy arrays is written for Python and requires roughly the same cost as a Python function call.
There are several ways to avoid this extra cost.
Cython does include an interface for C arrays, but the interface is not terribly convenient, so we will not discuss it here.
Fortunately, Cython includes a C-level interface for Numpy-like array objects.

A typed NumPy array can be declared like this:
\begin{lstlisting}
cimport numpy
...
cdef numpy.ndarray[dtype=double, ndim=2] X = ...
\end{lstlisting}
Or, more simply, as
\begin{lstlisting}
from numpy cimport ndarray as ar
...
cdef ar[double, ndim=2] X = ...
\end{lstlisting}
The \li{ndim} argument defaults to one and can be omitted when declaring a one-dimensional array, as in
\begin{lstlisting}
from numpy cimport ndarray as ar
...
cdef ar[double] X = ...
\end{lstlisting}
Declaring arrays in this manner allows us to access individual items in a NumPy array at roughly the same speed we could access items in a C array.

An additional argument, \li{mode}, may be used to specify whether the array is C-contiguous, Fortran-contiguous, full, or strided (\li{c}, \li{fortran}, \li{full}, and \li{strided} respectively).
An array is said to be C contiguous when neighboring values of the last dimension are closest together in memory.
Fortran-contiguous arrays store data in the opposite ordering with neighboring values in the first dimension closest together.
For a 2D array, C-contiguous ordering puts rows in continuous blocks of memory while Fortran-contiguous stores by column.
The memory layout of an array will determine an optimal way to iterate over that array.
For example, if an array is C-contiguous it is best to have the fastest varying indices run along its rows rather than its columns.
The memory layout of an array may be declared like this:
\begin{lstlisting}
from numpy cimport ndarray as ar
...
#C-contiguous
cdef ar[double, ndim=2, mode='c'] X = ...
\end{lstlisting}

Specifying the mode of the array does not add extra speed if the array is already laid out in the desired way, but it will raise an error if the array declared is not arranged correctly.
Not only do we have all the convenience of using a NumPy array with its various array operations, but we can also access single items in the array more quickly.
Unfortunately, this fast array access only works when accessing \textit{one item at a time} from the array.
Generally we can get around the slicing, but if we need to pass around NumPy array slices between the functions in the module, there can be a significant speed loss (these array slices are Python objects).
Cython has a \li{memoryview} object designed for this.
Memoryviews support the same fast indexing as the NumPy arrays and slices of memoryviews continue to support the optimized buffer access.
%% I'll demostrate this in the solutions
They are useful when you need to pass array slices between functions in your module.
They also work especially well when used in inline functions, and the corresponding syntax is relatively simple.
%%will demonstrate benefits of inlining as well
However, passing a memoryview object to a NumPy function becomes slower as the memoryview has to be converted to a NumPy array.
%%also shown in solutions
If you need to use NumPy array functions and pass memoryviews between functions you can define a \li{memoryview} object that views the same data as your array.
If for some reason you need to convert a memoryview to a NumPy array, you can use the \li{np.asarray} function, but that, as always, comes at a cost.
The syntax to use when declaring a memoryview object from an array \li{X} is:
\begin{lstlisting}
cdef double[:,:] Xview = X
\end{lstlisting}
If we know more about the memory layout of \li{X} we can also add that to the type declaration.
Knowing that \li{X} is C-contiguous (which is normally true for a newly initialized NumPy array) the following works:
\begin{lstlisting}
cdef double[:,::1] Xview = X
\end{lstlisting}
When the array is Fortran-contiguous we can use the following:
\begin{lstlisting}
cdef double[::1,:] Xview = X
\end{lstlisting}

It is also worth noting that many of the array operations in Cython can also be done using pointers.
The speed is roughly the same as with the optimized array lookups, but the code is often much less readable.
Cython does not allow you to dereference a pointer using the \li{*} syntax.
It requires square brackets \li{[ ]} instead.

\section*{Compiler Directives}
There are several compiler directives which may be passed to the Cython compiler to speed up array access.
By default, Cython checks to see if the indices used in array accesses are within the bounds of the array.
Cython also allows negative indexing the same way Python does.
These features incur some performance loss and may be removed after a program has been carefully debugged.
Compiler directives in Cython can be included as comments or as function decorators.
Directives included in comments will apply to the whole file, while function decorators will only apply to the function or method immediately following the decorators.
The comments to turn off bounds checking and negative indices are
\begin{lstlisting}
# cython: boundscheck=False
# cython: wraparound=False
\end{lstlisting}
To use the function decorators, first import the \li{cython} module by including the line \li{cimport cython} in your import statements.
The decorators are
\begin{lstlisting}
cimport cython
@cython.boundscheck(False)
@cython.wraparound(False)
\end{lstlisting}
When using these compiler directives, make absolutely sure that your program is \emph{not} using negative indices or accessing any array out of bounds.
Doing so could access and possibly modify or access some portion of memory that has not been allocated as part of an array.
That can either crash Python or cause unexpected behavior in a program.
It is usually best to only include these directives after debugging a program thoroughly.

Cython has many other compiler directives.
One to be aware of is the \li{cdivision} option.
In Python, the \li{\%} operator returns a number with the sign of the second argument, while C keeps the sign of the first argument.
In Python, \li{-1\%5} returns \li{4}, while in C, this returns \li{-1}.
Cython, by default, will behave like Python.
Cython will also check for zero division and raise a \li{ZeroDivisionError} when necessary.
This feature does have some cost.
If needed, you can make the \li{\%} operator behave like it would in C and turn off the check for zero division by setting \li{cdivision} to \li{True}.

\section*{Functions in Cython}
Type declarations can also be used for function arguments.
You can declare the types of the inputs for the function to ensure that it receives the right arguments.
\begin{lstlisting}
from numpy cimport ndarray as ar
def myfunction(ar[double] X, int n, double h, items):
    ...
\end{lstlisting}
Notice that the type declarations were not included for all of the arguments.
The untyped arguments are expected to be Python objects with the corresponding methods.
Computations involving these untyped arguments will use Python instead of C.
Keyword arguments are allowed for both typed and untyped arguments.

Cython also allows you to make C functions that are only callable within the C extension you are currently building.
These functions are declared using the same syntax as in Python, except the keyword \li{def} is replaced with \li{cdef}.
These functions can be called within the module, but are not actually imported into the Python namespace of the Python module.

Cython functions may be declared using the \li{cpdef} statement.
These functions are accesible within Cython without the cost of a Python function call, but can still be called from Python.
The return type for functions declared using \li{cdef} and \li{cpdef} may also be specified, for example:
\begin{lstlisting}
from numpy cimport ndarray as ar
cpdef int myfunction(ar[double] X, int n, double h, items):
    ...
\end{lstlisting}

\section*{Some Examples}
To show how to use Cython, we will take the dot product of two one-dimensional arrays.
This can be done in Python as follows:
\begin{lstlisting}
def pydot(A, B):
    tot = 0.
    for i in xrange(A.shape[0]):
        tot += A[i] * b[i]
    return tot
\end{lstlisting}
A C-compiled equivalent can be made using the following Cython code
\begin{lstlisting}
%%cython
from numpy cimport ndarray as ar
cimport cython

@cython.boundscheck(False)
@cython.wraparound(False)
def cydot(ar[double] A, ar[double] B):
    cdef double tot=0.
    cdef int i
    for i in xrange(A.shape[0]):
        tot += A[i] * B[i]
    return tot
\end{lstlisting}
This function may be timed by evaluating the following cell.
\begin{lstlisting}
from numpy.random import rand
n = 10000000
A = rand(n)
B = rand(n)
%timeit cydot(A, B)
\end{lstlisting}

Figure \ref{cython:dot} compares the timings of this new dot-product function with other possible implementations.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{dot.pdf}
\caption{The running times of the pure Python dot-product, the Cython based dot-product, and the dot-product built into NumPy which links to the Intel MKL.
The Cython version can run nearly as fast as the version built into NumPy.}
\label{cython:dot}
\end{figure}

Now we will do a more advanced example using memoryviews.
We will write functions which, given a two-dimensional array \li{A}, make a new array \li{B} such that
\li{B[i,j] = dot(A[i],A[j])}.

Here's a purely iteration-based solution in Python:
\begin{lstlisting}
def pyrowdot(A):
    B = np.empty((A.shape[0], A.shape[0]))
    for i in xrange(A.shape[0]):
        for j in xrange(i):
            temp = pydot(A[i], A[j])
            B[i,j] = temp
        B[i,i] = pydot(A[i], A[i])
    for i in xrange(A.shape[0]):
        for j in xrange(i+1, A.shape[0]):
            B[i,j] = B[j,i]
    return B
\end{lstlisting}

To do this same sort of thing in Cython we can compile the following file:

\begin{lstlisting}
import numpy as np
from numpy cimport ndarray as ar
cimport cython

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef inline cydot(double[:] A, double[:] B, int n):
    cdef double tot=0.
    cdef int i
    for i in xrange(n):
        tot += A[i] * B[i]
    return tot

@cython.boundscheck(False)
@cython.wraparound(False)
def cyrowdot(ar[double, ndim=2] A):
    cdef ar[double, ndim=2] B = np.empty((A.shape[0], A.shape[0]))
    cdef double[:,:] Aview = A
    cdef double temp
    cdef int i, j, n=A.shape[0]
    for i in xrange(n):
        for j in xrange(i):
            temp = cydot(Aview[i], Aview[j], n)
            B[i,j] = temp
        B[i,i] = cydot(Aview[i], Aview[i], n)
    for i in xrange(n):
        for j in xrange(i+1, n):
            B[i,j] = B[j,i]
    return B
\end{lstlisting}

This can also be done in NumPy by running \li{A.dot(A.T)}.
The timings of the Python and Cython versions of this function are shown in Figure \ref{cython:rowdot}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{rowdot.pdf}
\caption{The times of the Python, Cython, and NumPy (with MKL) versions of the \li{rowdot} function that we used as an example.
The arrays used for testing were $n\times 3$ where $n$ is shown along the horizontal axis.}
\label{cython:rowdot}
\end{figure}

\begin{problem}
Write a function in Python which takes the sum of a one-dimensional array by iterating over it.

Write three Cython versions: one using a typed for-loop to iterate over the array, another using a typed for-loop and optimized array access, and another using a typed for-loop, optimized array access, and special compiler directives to further speed up array access.

Compare the speed of the functions you just wrote, the builtin \li{sum()} function and NumPy's \li{sum()} function.
What do you see?
Notice that when you specify the types for your variables in ways that don't work well, you may actually slow things down.
\end{problem}

\section*{Using C Functions and Data Types in Cython}
Cython also allows you to easily interface between Python and C.
It comes with many of the basic math functions from the math library already implemented.
These functions can be imported using something like:
\begin{lstlisting}
from libc.math cimport fabs, sin, cos, ...
\end{lstlisting}
Notice that we imported \li{fabs}.
This is the absolute value function from C.
The 'f' in \li{fabs} is short for floating point, since this function operates on floating point numbers.
\li{min} and \li{max} are also renamed the same way.
These functions are good for large amounts of computation when we don't want to deal with the cost of using Python objects.
Cython also allows you to import other C functions and libraries.
It can make it much easier to use the libraries from Python.

\begin{problem}
In an earlier lab, you wrote a function to compute the LU decomposition of a nonsingular matrix.
Port it to Cython.
Use the typed for-loops, typed arrays (or memoryviews), and the additional compiler directives in your optimized solution.
You may assume, in this case, that you are only dealing with real, two-dimensional arrays of double precision floating point numbers.
Compare the speed of your new solution to the speed of the Python based version you wrote earlier.
\end{problem}

\begin{problem}
The code below defines a Python function which takes a matrix to the $n$th power.
Port it to Cython.
Write three different versions: one using typed arrays, and another using typed arrays with the special compiler directives.
\begin{lstlisting}
import numpy as np
def mymult(X, power):
    prod = np.empty_like(X)
    prod[:] = X
    temparr = np.empty_like(X[0])
    size = X.shape[0]
    for n in xrange(1, power):
        for i in xrange(size):
            for j in xrange(size):
                tot = 0.
                for k in xrange(size):
                    tot += prod[i,k] * X[k,j]
                temparr[j] = tot
            prod[i] = temparr
    return prod
\end{lstlisting}

Compare the speed of the Python function, the two functions you just wrote, and the \li{np.dot()} function.
The NumPy function should still be faster, but each of these solutions should be much faster than the pure Python version.
NumPy and SciPy do this computation and other computations by calling BLAS and LAPACK, very optimized Fortran libraries for linear algebra.
This is probably one of the best-optimized portions of NumPy and Scipy.
The difference in performance should be particularly clear in this case because of the high order of complexity of the algorithm.
\end{problem}

It is important to recognize that the choice of algorithm and fast implementation can both drastically affect the speed of your code.
A simple example is the tridiagonal algorithm which is used to solve systems of the form:
\[\begin{bmatrix}
b_0 & c_0 & 0 & 0 & 0 & \cdots & \cdots & 0 \\
a_0 & b_1 & c_1 & 0 & 0 & \cdots & \cdots & 0 \\
0 & a_1 & b_2 & c_2 & 0 & \cdots & \cdots & 0 \\
0 & 0 & a_2 & b_3 & c_3 & \cdots & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & c_{n-1} \\
0 & 0 & 0 & 0 & 0 & \cdots & a_{n-1} & b_n
\end{bmatrix}
\begin{bmatrix}
d_0\\
d_1\\
d_2\\
d_3\\
\vdots\\
\vdots\\
d_n
\end{bmatrix}
=
\begin{bmatrix}
x_0\\
x_1\\
x_2\\
x_3\\
\vdots\\
\vdots\\
x_n
\end{bmatrix}\]
The following Python code solves this system.
(\li{x} and \li{c} are modified in place)
The final result is stored in \li{x}, and \li{c} is used to store temporary values.
\begin{lstlisting}
def tridiag(a, b, c, x):
    #note: overrides c and x
    n = x.size
    temp = 0.
    c[0] = c[0] / b[0]
    x[0] = x[0] / b[0]
    for i in range(n - 2):
        temp = 1. / (b[i+1] - a[i]*c[i])
        c[i+1] *= temp
        x[i+1] = (x[i+1] - a[i] * x[i]) * temp
    x[n-1] = (x[n-1] - a[n-2] * x[n-2]) / (b[n-1] - a[n-2] * c[n-2])
    for i in range(n-2, -1, -1):
        x[i] = x[i] - c[i] * x[i+1]
\end{lstlisting}

\begin{problem}
Port the above code to Cython using typed for-loops, optimized array accesses and the special compiler directives.
Compare the speed of your new function with the pure Python version given above.
Now compare the speed of both of these functions with the \li{solve()} function in \li{scipy.linalg}.
For your first two functions a good starting point for computation will be to consider $1000000 \times 1000000$ sized systems and then adjust the size so you get good results on your particular machine.
When testing the SciPy algorithm, you will probably want to start with systems involving a $1000 \times 1000$ matrix and then go up from there.
Keep in mind that the SciPy function is heavily optimized, but that it uses a much more general algorithm.

What does this example tell you about the relationship between good implementation and proper choice of algorithm?
In this case, the LU decomposition method used by the SciPy function has complexity $O(n^3)$, while the tridiagonal algorithm is $O(n)$.
\end{problem}
