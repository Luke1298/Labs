\lab{Discrete HMM}{Discrete Hidden Markov Models}
\label{lab:hmm}
\objective{Understand how to fit discrete Hidden Markov Models.}

Given a discrete state-space Hidden Markov Model (HMM) with parameters $\lambda$ and an observation sequence $O$, we would like to answer three questions:
\begin{enumerate}
 \item What is $\mathbb{P}(O |\, \lambda)$? In other words, what is the likelihood that our model generated the observation sequence?
 \item What is the most likely state sequence to have generated $O$, given $\lambda$?
 \item How can we choose the parameters $\lambda$ that maximize $\mathbb{P}(O | \, \lambda)$?
\end{enumerate}
The answers to these questions are centered around the \emph{forward-backward} algorithm for HMMs.
For the second question, the approach taken in this lab will be to try to find the state sequence maximizing the expected number of correct states.
The third question is an example of \emph{unsupervised learning}, since we are attempting to learn (or fit) model parameters using data (the observation sequence $O$) that is devoid
of human-provided labels (the corresponding state sequence); the algorithm does not rely on human supervision or input.

We assume throughout this lab that the HMM has a discrete state space of cardinality $N$ and a discrete observation space of cardinality $M$.
In this context, $\lambda = \left( A, B, \mathbf{\pi} \right)$ where $A$ is a $N \times N$  column-stochastic matrix (the state transition model), $B$ is a $M \times N$ column-stochastic matrix (the
state-observation model), and $\mathbf{\pi}$ is a stochastic vector of length $N$ (the initial state distribution).
Further, $O$ is a vector of length $T$ with values in $\{1,2,\ldots,M\}$.

\begin{warn}
The mathematical exposition in the lab assumes the standard 1-based indexing of vectors and matrices.
It will be your job to carefully translate the various formulae into 0-based indexing for the Python code.
This means that in Python, your array containing the observation sequence $O$ will actually have values
in the set $\{0,1,\ldots,M-1\}$, so that they may be used to index the matrix $B$.
\end{warn}

Throughout this lab, we will be using the data found in the file {\tt declaration.txt}.
This file contains the text of the Declaration of Independence.
We will use the sequence of characters (after stripping out punctuation and converting everything to lower-case) as our observation sequence.
In order to convert the raw text into a useable data structure, we need to read in the file, process the string as necessary, and then map the characters to integer values.
We provide sample code below to accomplish this task:
\begin{lstlisting}
>>> import numpy as np
>>> import string

>>> with open("declaration.txt", 'r') as f: # read in the text
>>>     dec = f.read(-1).lower() # convert to lower-case

>>> # next, remove punctuation and newline characters
>>> dec = dec.translate(string.maketrans("",""), string.punctuation+"\n")

>>> # create a list of the unique characters in the text
>>> char_map = list(set(dec))

>>> # map each character to its index in char_map
>>> obs = []
>>> for char in dec:
>>>     obs.append(char_map.index(char))
>>> obs = np.array(obs)
\end{lstlisting}

\begin{problem}
To start off your implementation of the HMM, define a class using the following code.
You will be adding class methods throughout the remainder of the lab.
\begin{lstlisting}
class hmm(object):
    """
    Finite state space hidden markov model.
    """
    def __init__(self):
        """
        Initialize model parameters.
        """
        self.A = None
        self.B = None
        self.pi = None
\end{lstlisting}
\end{problem}
\subsection*{The Forward Pass}
Our first task is to efficiently compute $\log \mathbb{P}(O | \lambda)$.
We can do this using the \emph{forward pass} of the forward-backward algorithm.
We must take care to compute all values in a numerical stable way; we do this by properly scaling values as necessary.

We compute a scaled forward probability matrix $\widehat{\alpha}$ of dimension $T \times N$ as follows.
Let $\widehat{\alpha}_{i,:}, B_{i,:}$ denote the $i$-th rows of $\widehat{\alpha}$ and $B$, respectively, let $\odot$ denote the Hadamard (or entry-wise) product of arrays,
and let $\langle \cdot, \cdot \rangle$ denote the standard dot product.
Then
\begin{itemize}
 \item $c_1 = \langle \pi, B_{O_1,:}\rangle^{-1}$
 \item $\widehat{\alpha}_{1,:} = c_1(\pi\odot B_{O_1,:})$
 \item For $t = 2, \ldots, T$:
 \begin{itemize}[]
     \item $c_t = \langle A\widehat{\alpha}_{t-1,:}, B_{O_t,:}\rangle^{-1}$
	 \item $\widehat{\alpha}_{t,:} = c_t((A\widehat{\alpha}_{t-1,:})\odot B_{O_t,:})$
 \end{itemize}
\end{itemize}
The matrix $\widehat{\alpha}$ will be of use when fitting parameters, but we can compute the desired log probability using the scaling factors $c_t$ as follows:
\[
\log \mathbb{P}(O | \lambda) = -\sum_{t=1}^T \log c_t.
\]

\begin{problem}
Implement the forward pass by adding the following method to your class:
\begin{lstlisting}
def _forward(self, obs):
    """
    Compute scaled forward probability matrix and scaling factors.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence

    Returns
    -------
    alpha : ndarray of shape (T,N)
        The scaled forward probability matrix
    c : ndarray of shape (T,)
        The scaling factors c = [c_1,c_2,...,c_T]
    """
    pass
\end{lstlisting}
To verify that your code works, you can use the following toy HMM example:
\begin{lstlisting}
>>> # toy HMM example to be used to check answers
>>> A = np.array([[.7, .4],[.3, .6]])
>>> B = np.array([[.1,.7],[.4, .2],[.5, .1]])
>>> pi = np.array([.6, .4])
>>> obs = np.array([0, 1, 0, 2])
>>> h = hmm()
>>> h.A = A
>>> h.B = B
>>> h.pi = pi
>>> alpha, c = h._forward(obs)
>>> print -(np.log(c)).sum() # the log prob of observation
-4.6429135909
\end{lstlisting}
\end{problem}

\subsection*{The Backward Pass}
The backward pass of the forward-backward algorithm produces values that can be used calculate the most likely state sequence corresponding to an observation sequence.

We compute a scaled backward probability matrix $\widehat{\beta}$ of dimension $T \times N$ as follows.
\begin{itemize}
 \item $\widehat{\beta}_{T,i} = c_{T}$ for $i = 1,\ldots, N$
 \item $\widehat{\beta}_{t,:} = c_{t}A^T(B_{O_{t+1},:}\odot \widehat{\beta}_{t+1,:})$ for $t = T-1, \ldots, 1$
\end{itemize}
(Above, $A^T$ is the \emph{transpose} of $A$, not the $T$-th power of $A$.)

\begin{problem}
Implement the backward pass by adding the following method to your class:
\begin{lstlisting}
def _backward(self, obs, c):
    """
    Compute scaled backward probability matrix.

     Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence
    c : ndarray of shape (T,)
        The scaling factors from the forward pass

    Returns
    -------
    beta : ndarray of shape (T,N)
        The scaled backward probability matrix
    """
    pass
\end{lstlisting}
Using the same toy example as before, your code should produce the following output:
\begin{lstlisting}
>>> beta = h._backward(obs, c)
>>> print beta
[[ 3.1361635   2.89939354]
 [ 2.86699344  4.39229044]
 [ 3.898812    2.66760821]
 [ 3.56816483  3.56816483]]
\end{lstlisting}
\end{problem}

\subsection*{Computing the $\delta$ Probabilities}
Having implemented both parts of the forward-backward algorithm, we are closing in on the solution to question 3, namely that of fitting parameters.
At this stage, we combine the information accumulated in the forward-backward algorithm to produce a three-dimensional array $\widehat{\delta}$
of shape $(T-1)\times N \times N$ whose entries are related to $\mathbb{P}(\mathbf{x}_t = i, \mathbf{x}_{t+1} = j|\, O, \lambda)$.
The relevant formula is
\[
\widehat{\delta}_{t,i,j} = \frac{\widehat{\alpha}_{t,i}A_{j,i}B_{O_{t+1},j}\widehat{\beta}_{t+1,j}}{\sum_{k,l}\widehat{\alpha}_{t,k}A_{l,k}B_{O_{t+1},l}\widehat{\beta}_{t+1,l}}
\]
for $t = 1, \ldots, T-1$ and $=i,j = 1, \ldots, N$.

\begin{problem}
Add the following method to your class to compute the $\delta$ probabilities.
\begin{lstlisting}
def _delta(self, obs, alpha, beta):
    """
    Compute the delta probabilities.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence
    alpha : ndarray of shape (T,N)
        The scaled forward probability matrix from forward pass
    beta : nd   array of shape (T,N)
        The scaled backward probability matrix from backward pass

    Returns
    -------
    delta : ndarray of shape (T-1,N,N)
        The delta probability array
    """
    pass
\end{lstlisting}
While writing a triply-nested loop may be the simplest way to convert the formula into code,
it is possible to use array broadcasting to eliminate some of the loops, which will speed up your code.

Check your code by making sure it produces the following output, using the same toy example as before.
\begin{lstlisting}
>>> delta = h._delta(obs, alpha, beta)
>>> print delta
[[[ 0.14166321  0.0465066 ]
  [ 0.37776855  0.43406164]]

 [[ 0.17015868  0.34927307]
  [ 0.05871895  0.4218493 ]]

 [[ 0.21080834  0.01806929]
  [ 0.59317106  0.17795132]]]
\end{lstlisting}
\end{problem}

\subsection*{Choosing Better Parameters}
After running the forward-backward algorithm and computing the $\delta$ probabilities, we are now in a position to choose new parameters $\lambda' = (A', B', \pi')$
that increase the probability of observing our data, i.e.
\[
\mathbb{P}(O|\,\lambda') \geq \mathbb{P}(O|\,\lambda).
\]
The update formulas are given by
\begin{align*}
A'_{i,j} &= \frac{\sum_{t=1}^{T-1}\widehat{\delta}_{t,j,i}}{\sum_{t=1}^{T-1}\sum_{k=1}^N\widehat{\delta}_{t,i,k}}\\
B'_{i,j} &= \frac{\sum_{t=1}^{T-1}\sum_{k=1}^N\widehat{\delta}_{t,i,k}1_{\{O_t=j\}}}{\sum_{t=1}^{T-1}\sum_{k=1}^N\widehat{\delta}_{t,i,k}}\\
\pi'_i &= \sum_{j=1}^N \widehat{\delta}_{1,i,j}.
\end{align*}
\begin{problem}
Implement the parameter update step by adding the following method to your class.
\begin{lstlisting}
def _estimate(self, obs, delta):
    """
    Estimate better parameter values.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        The observation sequence
    delta : ndarray of shape (T-1,N,N)
        The delta probability array
    """
    # update self.A, self.B, self.pi in place
    pass
\end{lstlisting}
Verify that your code produces the following output on the toy HMM from before:
\begin{lstlisting}
h._estimate(obs, delta)
>>> print h.A
[[ 0.55807991  0.49898142]
 [ 0.44192009  0.50101858]]
>>> print h.B
[[ 0.44533551  0.76711248]
 [ 0.55466449  0.23288752]
 [ 0.          0.        ]]
>>> print h.pi
[ 0.18816981  0.81183019]
\end{lstlisting}
\end{problem}

\subsection*{Fitting the Model}
We are now ready to put everything together into a learning algorithm.
Given a sequence of observations, a maximum number of iterations $K$, and a convergence tolerance threshold $\epsilon$, we fit a HMM model using the following procedure:
\begin{itemize}
\item Randomly initialize parameters $\lambda = (A, B, \pi)$
\item Compute $\log \mathbb{P}(O |\, \lambda)$
\item For $i=1, 2, \ldots, K$:
\begin{itemize}
\item Run forward pass
\item Run backward pass
\item Compute $\delta$ probabilities
\item Update model parameters
\item Compute $\log \mathbb{P}(O |\, \lambda)$ according to new parameters
\item If change in log probabilities is less than $\epsilon$, break
\item Else, continue
\end{itemize}
\end{itemize}

The most convenient way to randomly initialize stochastic matrices is to draw from the Dirichlet distribution,
which produces vectors with nonnegative entries that sum to 1.
The following Python code initializes $A$, $B$, and $\pi$ using this technique:
\begin{lstlisting}
>>> # assume N and M are defined
>>> A = np.random.dirichlet(np.ones(N), size=N).T
>>> B = np.random.dirichlet(np.ones(M), size=N).T
>>> pi = np.random.dirichlet(np.ones(N))
\end{lstlisting}

\begin{problem}
Implement the learning algorithm by adding the following method to your class.
\begin{lstlisting}
def fit(self, obs, n, m, max_iter=100, tol=1e-3):
    """
    Use EM to fit model parameters to a given observation sequence.

    Parameters
    ----------
    obs : ndarray of shape (T,)
        Observation sequence on which to train the model.
    n : integer
        Cardinality of state space
    m : integer
        Cardinality of observation space
    max_iter : integer
        The maximum number of iterations to take
    tol : float
        The convergence threshold for change in log-probability
    """
    pass
\end{lstlisting}
\end{problem}

\begin{problem}
You are now ready to train a HMM using the Declaration of Independence data.
Use $N=2$ states and $M=27$ observation values (26 lower case characters and 1 whitespace character),
and the default values for \li{max_iter} and \li{tol}.

Once the learning algorithm converges, analyze the state-observation matrix $B$.
Note which rows correspond to the largest and smallest probability values in each column of $B$, 
and check the corresponding characters.
The code below prints the characters corresponding to the 6 largest and smallest values in each column of $B$:
\begin{lstlisting}
>>> # assume h has been trained to the declaration data
>>> # print characters corresponding to largest and smallest 6 values
>>> ind1 = np.argpartition(h.B[:,0], -6)[-6:] #indices of 6 highest probs
>>> ind2 = np.argpartition(-h.B[:,0], -6)[-6:] #indices of 6 lowest probs
>>> print np.array(letter_map)[ind1], np.array(letter_map)[ind2]

>>> ind1 = np.argpartition(h.B[:,1], -6)[-6:] #indices of 6 lowest probs
>>> ind2 = np.argpartition(-h.B[:,1], -6)[-6:] #indices of 6 lowest probs
>>> print np.array(letter_map)[ind1], np.array(letter_map)[ind2]
\end{lstlisting}
\end{problem}
It also turns out that
\begin{equation*}
\mathbb{P}(\mathbf{x}_{t} = i | O, \lambda) = \frac{\widehat{\alpha}_{t}(i)\widehat{\beta}_{t}(i)}{\sum_{j=1}^{N} \widehat{\alpha}_{t}(j)\widehat{\beta}_{t}(j)}
\end{equation*}
and so we can easily compute the expected state at time $t$ by
\begin{equation*}
\mathbf{x}_{t}^{*} = \argmax_{i} \widehat{\alpha}_{t}(i) \widehat{\beta}_{t}(i)
\end{equation*}



