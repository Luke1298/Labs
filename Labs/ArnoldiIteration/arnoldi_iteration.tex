\lab{Krylov Subspaces}{Finding Eigenvalues Using Iterative Methods}
\label{lab:kry_arnoldi}

\objective{Discuss simple Krylov Subspace Methods for finding eigenvalues and show some interesting applications.}

One of the biggest difficulties in computational linear algebra is the amount of memory needed to store a large matrix and the amount of time needed to read its entries.
Methods using Krylov subspaces avoid this difficulty by studying how a matrix acts on vectors, making it unnecessary in many cases to create the matrix itself.

More specifically, we can construct a Krylov subspace just by knowing how a linear transformation acts on vectors, and with these subspaces we can closely approximate eigenvalues of the transformation and solutions to associated linear systems.

The \emph{Arnoldi iteration} is an algorithm for finding an orthonormal basis of a Krylov subspace. 
Its outputs can also be used to approximate the eigenvalues of the original matrix.

\section*{The Arnoldi Iteration}
The order-$N$ Krylov subspace of $A$ generated by $\x$ is 
\[
\mathcal{K}_n(A, \x) = \{\x, A\x, A^2\x, \ldots, A^{n-1}\x\}.
\]
If the vectors $\{\x, A\x, A^2\x, \ldots, A^{n-1}\x\}$ are linearly independent, then they form a basis for $\mathcal{K}_n(A,\x)$. 
However, this basis is usually far from orthogonal, and hence computations using this basis will likely be ill-conditioned.

One way to find an orthonormal basis for $\mathcal{K}_n(A,\x)$ would be to use the modified Gram-Schmidt algorithm from Lab TODO on the set $\{\x, A\x, A^2\x, \ldots, A^{n-1}\x\}$.
More efficiently, the Arnold iteration integrates the creation of $\{\x, A\x, A^2\x, \ldots, A^{n-1}\x\}$ with the modified Gram-Schmidt algorithm, returning an orthonormal basis for $\mathcal{K}_n(A,\x)$. This algorithm is . . .





\begin{comment}
Before discussing the specific uses of the Krylov subspace in linear systems and eigenvalue problems, let us address a very
practical concern: how can we best compute a basis for the Krylov subspace? The obvious answer is simply to
calculate the vectors $x, Ax, A^2x, \ldots, A^{N-1} x$, which we can accomplish using only matrix-vector multiplication.
Straightforward though this may be, there is a major problem: $A^n x$ tends to converge to a dominant eigenvector of $A$
as $n$ gets large, and consequently these vectors become nearly parallel. Thus, the basis $\{x, Ax, A^2x, \ldots, A^{N-1} x\}$
is far from orthogonal, and matrix computations associated with this basis will likely be ill-conditioned and prone to
numerical instability. To redress this problem, we may think to apply the Gram-Schmidt orthogonalization process to the
basis, obtaining an orthonormal basis for the Krylov subspace that enjoys much better numerical properties. This turns out to
be a useful thought, and is the basis for the Arnoldi iteration.

You may recall from lab \ref{lab:QRdecomp} that the Modified Gram-Schmidt algorithm allows us to find an increasing number of orthogonal
vectors but does not require that we run the algorithm to is completion to find a full basis.
Our goal is to find an orthonormal set of vectors $q_1,\ldots,q_N$ having the same span as $x, Ax, A^2x, \ldots, A^{N-1} x$.
We start
 things off by setting
\[
q_1 = \frac{x}{\|x\|_2}.
\]
Now, assuming we have obtained $q_1,\ldots,q_n$, we obtain $q_{n+1}$ by projecting $A q_n$ onto the previous vectors,
subtracting out these projections, and then normalizing. To make this more precise, let $h_{i,n} = \langle q_i, A q_n\rangle$
for $i = 1,\ldots, n$. Subtract out these projections by calculating
\[
p_{n+1} = A^n x - \sum_{i=1}^n h_{i,n}q_i.
\]
Define $h_{n+1,n} = \|p_{n+1}\|_2$, and normalize $p_{n+1}$ by calculating
\[
q_{n+1} = \frac{p_{n+1}}{h_{n+1,n}},
\]
our next basis vector. This procedure is outlined (in slightly more Python-friendly notation) in Algorithm \ref{alg:arnoldi_iteration}.

Perhaps you noticed a slight discrepancy between the Arnoldi iteration as described above, and the usual Gram-Schmidt procedure.
Specifically, you might have expected to compute $q_{n+1}$ by projecting $A^n x$, rather than $A q_n$, onto the previous vectors.
Thankfully, it is straightforward to show that our algorithm produces a valid orthonormal basis for the Krylov subspace,
despite this difference. And because of this detail, we do not need to compute and store the original Krylov basis
$x, Ax, \ldots, A^{N-1}x$.
Additionally, each iteration only requires one matrix-vector calculation, and the individual entries in $A$ are never
referenced or modified. Thus, even if the matrix $A$ is very large in theory, as long as we have a reasonably efficient
subroutine to calculate $Ax$ for any vector $x$, the Arnoldi iteration is computationally tractable.

This algorithm produces an othonormal basis $q_1,\ldots,q_N$ for the order-$N$ Krylov subspace generated by $A$ and $x$, as well
as a collection of numbers $h_{i,j}$. If we define a matrix $H_N$ whose $i,j$'th entry is $h_{i,j}$ for $i \leq j+1$ and
is $0$ otherwise, we now have an upper Hessenberg matrix.
Recall that an upper Hessenberg matrix has the property that all entries below the first subdiagonal
are equal to zero. Any square matrix is unitarily similar to an upper Hessenberg matrix. Dealing with a Hessenberg matrix is
often more convenient than dealing with a general matrix, especially when it comes to finding eigenvalues or solving systems
of equations, since efficient algorithms designed for these types of matrices exist.
It turns out that there is a Hessenberg factorization of $A$, given by
\[
A  = QHQ^*,
\]
where $Q$ is a unitary matrix and $H$ is upper Hessenberg such that the first $N$ columns of $Q$ are $q_1,\ldots,q_N$, and
the upper left $N \times N$ submatrix of $H$ is equal to $H_N$. Hence, the Arnoldi iteration provides a connection between
the Krylov subspace and the Hessenberg factorization of a matrix. Each step in the Arnoldi iteration can be thought of as computing
another step in the Hessenberg reduction of $A$. Each $H_N$ is really just the $N \times N + 1$ upper-left block of $H$.
Solving eigenvalue problems or systems of equations for
a general square matrix can thus be reduced, via Arnoldi iteration, to solving these problems for a Hessenberg matrix,
a much easier task.

At this point, we can view the Arnoldi iteration as a means to compute an orthonormal basis for a Krylov subspace, or
alternatively, to compute a partial Hessenberg factorization of a matrix.
But in Lab \ref{lab:Canonical_Transformations}, we discussed how orthogonal transformations can be used to transform a matrix to Upper
Hessenberg form.
We were able to find the eigenvalues of such Upper Hessenberg matrices in Lab \ref{lab:EigSolve}.
So what have we really gained by this new approach?
These previous approaches were based on matrix-matrix multiplication and required us to manipulate individual entries of the matrix.
The Arnoldi iteration avoids this and relies only on our ability to calculate matrix-vector multiplication.
Further, our present approach will allow us to compute only a partial Hessenberg factorization. This is advantageous when, as
is often the case, the behavior and properties of a matrix can be well-approximated by only a small portion of its Hessenberg
form.
\end{comment}

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{arnoldi}{$b, A, k, tol=1E-8$}
	\State $Q \gets \text{empty}\left(b.size, k+1\right)$	\Comment{Some initialization steps}
	\State $H \gets \text{zeros}\left( k+1, k\right)$
	\State $Q_{[:,0]} = b$							
	\State $Q_{[:,0]} /= \|Q_{[:,0]}\|_2$						
	\For{$j=0$, $j<k$}							\Comment{Perform the actual iteration.}
		\State $Q_{[:,j+1]} = AQ_{[:,j]}$		
		\For{$i=0$, $i<j+1$}					\Comment{Modified Gram-Schmidt.}
			\State $H_{[i,j]} = \langle Q_{[:,i]}, Q_{[:,j+1]}\rangle$		
			\State $Q_{[:,j+1]} -= H_{[i,j]} Q_{[:,i]}$
		\EndFor
		\State $H_{[j+1,j]} = \|Q_{[:,j+1]}\|_2$			\Comment{Set subdiagonal element of $H$.}
            \If{$|H_{[j+1,j]}|<tol$}					\Comment{Stop if $\|Q_{[:,j+1]}\|_2$ is too small.}
			\State \pseudoli{return} $H_{[:j+1,:j+1]}$, $Q_{[:,:j+1]}$
		\EndIf
		\State $Q_{[:,j+1]} /= H_{[j+1,j]}$				\Comment{Normalize $q_{j+1}$.}
	\EndFor
	\State \pseudoli{return} $H_{[:-1, :]}$, $Q$			\Comment{Return $H_{k}$.}
\EndProcedure
\end{algorithmic}
\caption{The Arnoldi Iteration}
\label{alg:arnoldi_iteration}
\end{algorithm}

\begin{warn}
To avoid errors involving the casting of complex numbers to real numbers make the datatypes of $H$ and $Q$ complex.
Make sure you account for the use of complex numbers when computing the norm.
You will also want to use the complex version of the \li{sqrt} function.
This function is in the built in \li{cmath} library.
\end{warn}

Notice that in Algorithm \ref{alg:arnoldi_iteration}, $k$ is the number of times to multiply by $A$.
This will result in a dimension $k+1$ Krylov Subspace.

\begin{problem}\label{prob:arnoldi}
Write a Python function that performs the Arnoldi iteration given a nonzero starting vector $b$, a function to multiply a
vector on the left by some matrix $A$, and a number $n$ of steps to perform.
Also have it accept a tolerance parameter that defaults to \li{1E-8}.
Have it return the computed $H_n$ and $Q_n$.
\end{problem}

% I think you can use la.norm on complex numers; no need to hard code.
% np.vdot will take a complex inner product; otherwise conjugate an arg before passing to np.inner.

\begin{info}
Depending on the matrix $A$, the Arnoldi iteration may end quickly.
This happens when there is a subspace $G$ that is fixed under left multiplication by $A$.
In this particular case, the matrix $H_k$ exactly matches the way $A$ acts on $G$.
This means that the eigenvalues of $H_k$ are eigenvalues of $A$.
The eigenvectors of $H$ will also be eigenvectors of $A$ after the change of basis defined by $Q_k$.
\end{info}

% Make a 3D plot showing the projection. Label the initial vector x and Ax.

\section*{Finding Eigenvalues Using Arnoldi iteration}

It is relatively easy to understand how Arnoldi iteration finds eigenvalues of a matrix.
In simple terms, the eigenvalues of the $H_k$ converge to the eigenvalues of $A$.
The eigenvalues of the $H_k$ are called Ritz values.
This can happen very quickly, but, in general, the rate of convergence depends on the matrix $A$.
This works because each $H_k$ is a kind of lower-dimensional approximation of $A$.
In adding the projection of $A q_n$ orthogonal to $q_0, \ldots, q_n$, we are really including the only portion
of the space spanned by the image of $q_0, \dots, q_n$ under $A$ that is not already included in their span.
Generally speaking, the Ritz values converge most quickly to the eigenvalues of largest magnitude.
Convergence is also faster for eigenvalues that are relatively far from the other eigenvalues of $A$.
Figures \ref{fig:arnoldi_random_eig_conv} and \ref{fig:arnoldi_random_val_conv} show the convergence of the
Ritz values to the largest eigenvalues of some different matrices.

\begin{figure}
\includegraphics[width=\textwidth]{rand_eigs_conv.pdf}
\caption{The convergence of the Ritz values to the largest eigenvalues of a matrix with random eigenvalues between $0$ and $1$.}
\label{fig:arnoldi_random_eig_conv}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{rand_vals_conv.pdf}
\caption{The convergence of the Ritz values to the largest eigenvalues of a matrix with random entries between $0$ and $1$.
Matrices of this form generally have a single isolated eigenvalue that is much larger than the rest.
It can be seen here that the Ritz values converge to this eigenvalue much more quickly.}
\label{fig:arnoldi_random_val_conv}
\end{figure}

\begin{problem}\label{prob:ritz}
Finish the following function that computes the Ritz values of a matrix.
\begin{lstlisting}
def ritz(Amul, dim, k, iters):
    ''' Find the `k' Ritz values with largest real part of the linear operator defined by `Amul'.
    
    INPUTS:
    Amul    - A function handle. Should describe a linear operator on 
              R^(dim).
    dim     - The dimension of the space on which `Amul' acts.
    numvals - The number of Ritz values to return.
    iters   - The number of times to perform the Arnoldi iteration. Must 
              be between `numvals' and `dim'.
    
    RETURN:
    Return the `k' Ritz values with largest real part of the operator defined by `Amul.' 
    '''
\end{lstlisting}
\end{problem}

One application of the Arnoldi iteration is to find the eigenvalues of linear operators that are too large to store in memory.
For example, if an operator acts on $\mathbb{C}^{2^{20}}$, then its matrix representation contains $2^{40}$ complex values.
Storing such a matrix would require 64 terabytes of memory!

An example of such an operator is the Fast Fourier Transform, claimed by SIAM to be one of the top algorithms of the century [TODO: cite!].
The Fast Fourier Transform is used in many applications, including oil hunting and mp3 compression.


\begin{problem}
\label{prob:fourier_eigs}
The eigenvalues of the Fast Fourier Transform are known to be $\{ -1, 1, -i, i \}$.
Use your function \li{ritz()} from Problem \ref{prob:ritz} to approximate the eigenvalues of the Fast Fourier Transform.
Set \li{k} to be 10 and set \li{dim} to be $2^{20}$.
For the argument \li{Amul}, use the \li{fft} function from \li{scipy.fftpack}.
\end{problem}

The Arnoldi iteration for finding eigenvalues is implemented in SciPy with the function \li{scipy.sparse.linalg.eigs()}.
This function has many more options than the implementation we wrote in Problem \ref{prob:ritz}.
In this example, the keyword argument \li{k=5} specifies that we want five Ritz values.
Note that even though this function comes from the \li{sparse} library in SciPy, we can still call it on regular NumPy arrays.

\begin{lstlisting}
>>> B = np.random.rand(10000).reshape(100, 100)
>>> sp.sparse.linalg.eigs(B, k=5, return_eigenvectors=False)
array([ -1.15577072-2.59438308j,  -2.63675878-1.09571889j,
        -2.63675878+1.09571889j,  -3.00915592+0.j        ,  50.14472893+0.j ])
\end{lstlisting}


\subsection*{Convergence}

\begin{problem}
Finish the following function to visualize the convergence of the Ritz values.
\begin{lstlisting}
def plot_ritz(Amul, dim, numvals, iters):
    ''' Plot the Ritz values of the linear operator defined by `Amul'.
    
    INPUTS:
    Amul    - A function handle. Should describe a linear operator on 
              R^(dim).
    dim     - The dimension of the space on which `Amul' acts.
    numvals - The number of Ritz values to plot.
    iters   - The number of times to perform the Arnoldi iteration. Must 
              be between `numvals' and `dim'.
    
    Creates a plot with the number of Arnoldi iterations k on the x-axis, and the Ritz values of the Hessenberg approximation H_k on the y-axis.
    '''
    \end{lstlisting}
Hints:
\begin{enumerate}
\item Before saving the Ritz values of $H_k$, use \li{np.sort()} to ensure that they are in a canonical order.
\item It is not efficient to call your function from Problem \ref{prob:ritz} for increasing values of \li{iters}. If your code takes too long to run, consider integrating your solutions to Problems \ref{prob:arnoldi} and \ref{prob:ritz} with the body of this function.
\end{enumerate}
Run your function on these examples. See the plots below.
TODO: output real and complex plots separately
\end{problem}


% Once the eigenvalue lab has been rewritten, have them use their own solver to find the eigenvalues of the $H_k$.

% This problem is very ill-conditioned.
\begin{comment}
\begin{problem}
Finding the roots of a polynomial can be represented as an eigenvalue problem.
Finding the roots of a monic polynomial (a polynomial with leading coefficient 1) $p = c_0 + c_1 x + \dots + c_{n-1} x^{n-1} + x^n$ is equivalent to finding the eigenvalues of the matrix
\[C = \begin{bmatrix}
0 & 0 & \dots & 0 & -c_0 \\
1 & 0 & \dots & 0 & -c_1 \\
0 & 1 & \dots & 0 & -c_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \dots & 1 & -c_{n-1} \end{bmatrix}\]
This matrix is called the companion matrix of the polynomial $p$.
As it happens, every matrix is similar to the companion matrix of its characteristic polynomial, but we won't use that fact here.

The following is a function that, given an array containing the coefficients $c_0, c_1, \dots, c_{n-1}$ for a monic polynomial $p$, performs matrix multiplication by the corresponding companion matrix.
\begin{lstlisting}
def companion_multiply(c, u):
    v = np.empty_like(u)
    v[0] = - c[0] * u[-1]
    v[1:] = u[:-1] - c[1:] * u[-1]
    return v
\end{lstlisting}

Use the Arnoldi iteration to estimate the five zeros of largest norm of a degree $1000$ monic polynomial with randomly chosen coefficients (the leading coefficient still needs to be 1).
Run $50$ steps of the Arnoldi iteration.
Compare your results with the roots of the polynomial computed using NumPy's \li{poly1d} class.
This computation can be done like this (where \li{c} is the array of random coefficients for the polynomial)
\begin{lstlisting}
p = np.poly1d([1] + list(c[::-1]))
roots = p.roots
# Now sort by absolute value from largest to smallest
roots = roots[np.absolute(roots).argsort()][::-1]
\end{lstlisting}
How close are the first few zeros of largest norm?
\end{problem}
\end{comment}

\section*{Lanczos Iteration(Optional)}

Depending on the symmetry of the problem we may be able to make the Arnoldi iteration more efficient.
Consider the case that $A$ is symmetric.
This means that the matrx $H$ is both Upper Hessenberg and symmetric, so it is tridiagonal.
Since $H$ is tridiagonal, we \textit{should} have that each $A q_n$ is orthogonal to $q_0, \dots, q_{n-2}$.
This means that storage of all the columns of $Q_k$ is no longer necessary.
We can run the entire algorithm while storing only the previous two columns of $Q$ that we have computed.
We can also represent $H$ as two vectors: a vector $\alpha$ storing the values along the main diagonal of $H$, and a vector $\beta$ storing the values in the first subdiagonal (the values in the first superdiagonal are the same).
This change in the way things are stored allows Algorithm \ref{alg:arnoldi_iteration} to be simplified to Algorithm \ref{alg:lanczos_iteration}.
Algorithm \ref{alg:lanczos_iteration} is known as the Lanczos iteration.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{lanczos}{$b, Amul, k, tol=1E-8$}
	\State $q_0 \gets 0$								\Comment{Some initialization}
	\State $q_1 \gets \frac{b}{\|b\|_2}$
	\State $\alpha \gets \text{empty}\left(k\right)$
	\State $\beta \gets \text{empty}\left(k\right)$
	\State $\beta_{-1} = 0$
	\For{$i=0$, $i<k$}									\Comment{Perform the iteration.}
		\State $z \gets Amul\left(q_1\right)$					\Comment{$z$ is a temporary vector to store $q_{i+1}$.}
		\State $\alpha_i = \langle q_1, z \rangle$				\Comment{$q_1$ is used to store the previous $q_i$.}
		\State $z -= \alpha_i q_1 + \beta_{i-1} q_0$				\Comment{$q_0$ is used to store $q_{i-1}$.}
		\State $\beta_{i} = \|z\|_2$						\Comment{Initialize $\beta_i$.}
		\If{$\beta_i<tol$}								\Comment{Stop if $\|q_{i+1}\|_2$ is too small.}
			\State \pseudoli{return} $\alpha [: i+1]$, $\beta [: i]$
		\EndIf
		\State $z /= \beta_i$
		\State $q_0, q_1 = q_1, z$						\Comment{Store new $q_{i+1}$ and $q_i$ on top of $q_1$ and $q_0$.}
	\EndFor
	\State \pseudoli{return} $\alpha$, $\beta [: -1]$
\EndProcedure
\end{algorithmic}
\caption{The Lanczos Iteration}
\label{alg:lanczos_iteration}
\end{algorithm}

\begin{problem}
\label{prob:lanczos}
Write a Python function that performs the Lanczos iteration.
Have it accept a starting vector $b$, a function $Amul$ that computes $A x$ for any vector $x$, a number $k$ of iterations to perform, and an optional argument $tol$ that defaults to \li{1E-8}.
\end{problem}

In its most basic form the Lanczos iteration is not stable.
In exact arithmetic the vectors $q_i$ are exactly orthogonal, but in the presence of roundoff error this may be absolutely false.
In imprecise arithmetic, it is possible for the $q_i$ to suffer from so much roundoff error that \textit{they may no longer even be linearly independent}.
There are a variety of modifications to the Lanczos iteration that address this instability.
The library used for Lanczos iteration in Scipy uses an algorithm called the Implicitly Restarted Lanczos Method.
We will not discuss these algorithms in detail here.

% If needed we could make a separate lab on the Lanczos iteration and the Implicitly Restarted Lanczos Method.
% There isn't time or space here for it though.

\begin{problem}
The following code performs matrix multiplication by a tridiagonal symmetric matrix.
It accepts vectors $a$ and $b$ and $u$.
$a$ stores the entries in the main diagonal of the matrix.
$b$ stores the entries in the first sub/superdiagonal.
The function returns the image of $u$ under the matrix represented by $a$ and $b$.

\begin{lstlisting}
def tri_mul(a, b, u):
    v = a * u
    v[:-1] += b * u[1:]
    v[1:] += b * u[:-1]
    return v
\end{lstlisting}

Use the Lanczos iteration function you wrote for Problem \ref{prob:lanczos} to estimate the $5$ largest eigenvalues of a symmetric tridiagonal matrix $A$ with random values in its nonzero diagonals (i.e. make $a$ and $b$ random).
For demonstration purposes, let $A$ be $1000 \times 1000$.
Perform $100$ iterations.
Compare the $5$ eigenvalues of largest absolute value with the $5$ Ritz values of largest norm.
How do they compare?

Try running your simulation a few times for different vectors $a$ and $b$.
You may notice that, occasionally, the largest eigenvalue is repeated in the Ritz values.
This happens because of the lack of orthogonality between the vectors used in the Lanczos iteration.
These erroneous eigenvaleus are called ``ghost eigenvalues."
They generally converge to actual eigenvalues of the matrix and can make the multiplicity of an eigenvalue look higher than it really is.
\end{problem}

\section*{Arnoldi Iteration in SciPy}

SciPy interfaces with a Fortran library called ARPACK that has good implementations of the Arnoldi and Lanczos algorithms.
The Arnoldi iteration is found in \li{scipy.sparse.linalg.eigs}.
The Implicitly Restarted Lanczos iteration is found in \li{scipy.sparse.linalg.eigsh}
These functions allow you to find either the largest or smallest eigenvalues of a sparse or dense matrix.
The function \li{scipy.sparse.linalg.svds} uses the Implicitly Restarted Lanczos iteration on $A^* A$ to find singular values.

\begin{problem}
In Lab \ref{lab:MarkovGraph} we discussed how to find the Laplacian Matrix of a graph.
The second-smallest eigenvalue of a graph is known as the ``Fiedler Value" or the ``algebraic connectivity."
The algebraic connectivity of a graph is positive if the graph is connected and zero if it is not.
In general, the multiplicity of the eigenvalue $0$ in the Laplacian matrix of a graph is the number of connected components of that graph.
The following code constructs the Laplacian matrix of a graph composed of a line of nodes.
The matrix is stored in \li{dia_matrix} format.
\begin{lstlisting}
import scipy.sparse as ss
m = 1000
d = np.ones(m)
d[1:-1] += np.ones(m-2)
l = ss.diags([-np.ones(m-1), d, -np.ones(m-1)], [-1, 0, 1])
\end{lstlisting}
Use the \li{eigsh} function to verify that this graph is connected.
You should look at documentation for the \li{scipy.sparse} library and find the options you need to use.
For proper convergence you will want to leave the number of eigenvalues computed at its default value.

The following code constructs the Laplacian matrix the same graph as before except that a single edge has been removed.
\begin{lstlisting}
m = 1000
cut = 500
d = np.ones(m)
d[1:-1] += np.ones(m-2)
d1 = -np.ones(m-1)
d1[cut] = 0
d[[cut, cut+1]] =1
l = ss.diags([d1, d, d1], [-1, 0, 1])
\end{lstlisting}
Verify that this graph is not connected.
\end{problem}
