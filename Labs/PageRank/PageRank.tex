\lab{PageRank Algorithm}{PageRank Algorithm}
\label{lab:PageRank}
\objective{Explain the basics of the PageRank Algorithm, and some of its applications}

When you enter keywords into Google's search engine, Google finds every page containing your keywords and lists the pages in order of their \emph{rank}.
The rank of a page reflects many factors, including how often the page is visited and how connected it is to other pages.
As of 2013, the PageRank algorithm is one of over 200 algorithms that Google uses to determine the rank of a website.
Named for Larry Page, cofounder of Google, this algorithm ranks pages based on how connected they are to the rest of the internet.

\section*{The internet as a graph}
The PageRank algorithm models the internet with a directed graph. 
Each webpage is a node, and there is an edge from node $i$ to node $j$ if page $i$ links to page $j$.
Let $\In(i)$ be the websites linking to page $i$ and let $\Out(i)$ be the websites that page $i$ links to. 
That is, $\In(i)$ is the set of nodes with an arrow to node $i$, and $\Out(i)$ is the set of nodes with an arrow from node $i$.

An example is illustrated in Figure \ref{TODO}.
In this example, we would intuitively say that page x is not very connected to other pages, while page y is very connected. 
Moreover, because many pages link to page x, we might assume that it is a better webpage, and so pages linked to by page x are more likely to be relevant than pages linked to by page y.
The PageRank makes these intuitive notions precise.

\section*{The PageRank algorithm}
The PageRank algorithm assumes that a surfer chooses a starting webpage randomly.
Then, if the surfer is at page $i$, they randomly select a page from $\Out(i)$ to visit next.
This means that the surfer's chance of being on page $i$ at time $t$ is determined by where they were at time $t-1$.

Suppose the internet has $N$ webpages and let $P(i,t)$ be the likelihood that the surfer is on page $i$ at time $t$.
Then the probabilities $P(i,t)$ are given by
\begin{equation}\label{equ:pr1}
p(i,0)=\frac{1}{N} \qquad p(i,t+1) = \sum_{j \in \In(i)} \frac{p(j,t)}{\Out(j)}.
\end{equation}

For example, in Figure \ref{} we have $N=?$ and . . . . .(TODO: insert example)

\subsection*{Refining the model: pages with no outbound links}
Model these pages as if they link to every other page (TODO: why?)

\subsection*{Refining the model: adding boredom}
The equations in \eqref{equ:pr1} describe the surfer's habits as a Markov chain.
However, the model is more realistic if we add the assumption that the surfer sometimes gets bored and randomly picks a new starting page.
We will denote the probability that a surfer stays interested at step $t$ by a constant $d$, called the \emph{damping factor}.
Then the probability that the surfer gets bored at time $t$ is $1-d$.
Accounting for boredom, the formulas in \eqref{equ:pr1} become
\begin{equation}\label{equ:pr2}
p(i,0)=\frac{1}{N} \qquad p(i,t+1) = \frac{1-d}{N} + d\sum_{j \in \In(i)} \frac{p(j,t)}{\Out(j)}.
\end{equation}
TODO: in hw, use d=.85.


\subsection*{Defining page rank}
As given by the PageRank algorithm, the \emph{rank} of page $i$ is
\[p(i) = \lim_{t\to \infty} p(i,t).\]
In other words, the page ranks are the steady state of the modified Markov chain defined in \eqref{equ:pr2}.

\subsection*{Matrix form of the PageRank algorithm}
We can rewrite \eqref{equ:pr2} as the matrix equation
\[
\mathbf{p}(0)=\frac{1}{N}\mathbf{1} \qquad \mathbf{p}(t+1) = dK\mathbf{p}(t) + \frac{1-d}{N}\mathbf{1}
\]
where $\mathbf{p}(t)=(p(1,t), p(2,t), \ldots, p(N,t))^T$ and $\mathbf{1} = (1,\ldots, 1)^T$, and $K$ is defined by
\[K_{ij} = \begin{cases} \frac{1}{\Out(j)} & \mbox{ if j links to i} \\
	0 & \mbox{ otherwise.} \end{cases}\]




Mathematically, the page rank is the steady state probability of our modified Markov chain.
We can rewrite this as a matrix equation:
\[R(t+1) = d K R(t) + \frac{1-d}{N} \begin{pmatrix}1\\\vdots\end{pmatrix}\]
Where $R_i(t) = PR(i,t)$.
The matrix $K$ is defined by:
\[K_{ij} = \begin{cases} \frac{1}{L(j)} & \mbox{ if j is linked to i} \\
	0 & \mbox{ otherwise} \end{cases}\]

We can write $K$ neatly as follows:
\[K = (B^{-1}A)^T\]

Where $A$ is the adjacency matrix for the directed graph (the edges are links) and $B$ is the diagonal matrix containing the number of links leaving each vertex.
Remember that we have to modify our graph so that any page without outbound links points to every other page.
Otherwise these definitions don't make sense (for example $B^{-1}$ will not be well-defined).
In practice, you should \textit{never} construct a diagonal matrix, invert it, then do matrix multiplication.
That would be a series of very computationally expensive and unnecessary operations.
Store the diagonal entries of $B$, compute the inverse by taking their reciprocal, then perform the matrix multiplication by multiplying each of the rows of $A$ by each of the corresponding  entries of the diagonal of $B^{-1}$.
These operations are mathematically equivalent versions of the inversion and multiplication, but they are much more efficient.

A useful way to think of this algorithm is to think in terms of voting.
Every link that I place on my web page is a vote for the web page that it points to.
Additionally, if I am more prestigious then my votes count for more.

\begin{problem}
Included with this lab is a sample data set.
There is a comment at the top.
Each of the remaining lines represents a link going from one web page to another.
Each line consists of two integers.
The first is the index of the web page containing the link.
The second is the index of the web page pointed to by the link.
Write a function that constructs the adjacency matrix by reading in the data from file.
Store the data in a \li{scipy.sparse.dok_matrix}.

Recall that you can open a file (in read mode) using an \li{with} construct
\begin{lstlisting}
with open(filename, 'r') as f:
    # do stuff here.
\end{lstlisting}
Once you have opened the file you can iterate through the lines of the file by doing something like
\begin{lstlisting}
for L in f:
    # 'L' iterates through each line of 'f'
    # You can do stuff with the lines here.
\end{lstlisting}
For each line, you will want to split the line into its space-separated parts.
This can be done with something like \li{l.strip().split()}.
The \li{strip()} method removes endline characters and the \li{split()} method splits the string into the space-separated parts.
Once you have read in the line, you can use a try-except block to avoid issues with errors that occur with the strings you obtain 
\end{problem}

There are several ways to solve for $\lim_{t \to \infty} R(t)$.
One option is simply iterate the equation for $R(t)$ until $\norm{R(t)-R(t-1)}$ is sufficiently small.

\begin{problem}
\label{prob:pagerank_dense_iter}
Write an implementation of the page rank algorithm that computes the steady state $R$ iteratively, as described above.
Your function should accept a file name for the data, a floating point value for $d$, and a size \li{n} that defaults to \li{None}.
When the size is not None, your function should only consider the upper left $n \times n$ portion of the array.
\end{problem}

Another method for computation assumes a steady state $R$ and solves the resulting system:
\[(I-dK)R = \frac{1-d}{N} \begin{pmatrix}1\\\vdots\end{pmatrix}\]
We won't have you implement this version of the algorithm here.

Another way to solve this problem is to make it into an eigenvalue problem.
Since $R$ can be viewed as a vector of probabilities we can rewrite $\left(\begin{smallmatrix}1\\\vdots\end{smallmatrix}\right) = E R$, where $E$ is a matrix of all ones.
Thus the equation becomes:
\[R = (dK + \frac{1-d}{N}E)R\]
Since $(dK + \frac{1-d}{N}E)$ will be a strictly positive stochastic matrix (each column sums to one) the Perron-Frobenius theorem guarantees that this eigenvalue equation corresponds to the largest magnitude eigenvalue and that the eigenvector is unique.
This means that, by finding the eigenvector corresponding to the largest eigenvalue of $(dK + \frac{1-d}{N}E)$, we can find $R$.
The first option for finding $R$ (the iterative approach) is equivalent to using a numerical method called power iteration to find the eigenvalues of this system.
As we have formulated the eigenvalue problem here, it cannot be stored easily in one of SciPy's sparse matrix types, but it is still possible to work with eigenvalue solvers that exploit the symmetries of this matrix.

\begin{problem}
Write another version of the algorithm that uses the eigenvalue solver in \li{scipy.linalg} to compute the page rank within a group of web pages.
You will want to solve the eigenvalue problem above, then return the eigenvector corresponding to the eigenvalue of largest magnitude.
It should accept the same inputs as your solution to Problem \ref{prob:pagerank_dense_iter}.
\end{problem}

The computed value for $R$ gives the page rank associated with each web page.

This algorithm has been used in several settings (primarily in academia).
For example, it has been used to rank graduate institutions, to rank impact factor of journals and has even been used in some biological applications.

Let's try implementing the Page Rank algorithm on a small subset of the internet.
The SNAP graph library includes a variety of medium sized data sets that can be used for analysis.
Their data sets can be found at \url{http://snap.stanford.edu/data/index.html}.
Download one of the data sets there so you can use it to test your page ranking algorithm.

The \li{matplotlib.pyplot.spy} command of this adjacency matrix yields the plot shown in figure \ref{fig:WebSparse}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{sparse_web.png}
\caption{\li{spy} command on the adjacency matrix corresponding to the websites supported by Notre Dame University in 1999.
Data was taken from the SNAP datasets.}
\label{fig:WebSparse}
\end{figure}

\begin{problem}
Use your previous implementations of the Page Rank algorithm to calculate the Page Rank of the first 3500 web pages in the data set you have downloaded.
Which page has the highest page rank, and what is the value of the page rank?
\label{prob:pg_calc}
\end{problem}

\begin{problem}
The iterative method allows for an easy implementation using sparse matrices.
Write an implementation of the iterative Page Rank algorithm that uses sparse matrices.
Use it to compute the Page Ranks for all the pages in the data set.
Which page has the highest rank?

Hint: You will want to convert your sparse matrix to a \li{csc_matrix} or a \li{csr_matrix} to perform the computations.
\end{problem}
