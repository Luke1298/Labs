\lab{Metropolis Algorithm}{Metropolis Algorithm}
\objective{Understand the basic principles of the Metropolis algorithm and apply these ideas to the 
Ising Model.}

\section*{The Metropolis Algorithm}
Sampling from a given probability distribution is an important task in applications throughout the sciences.
When these distributions are complicated, as is often the case when modeling the world, direct sampling methods 
can become difficult, as they might involve computing high-dimensional integrals. 
The Metropolis algorithm is an effective method to sample from many distributions, requiring only that we 
be able to evaluate the probability density function up to a constant of proportionality. In particular,
the Metropolis algorithm does not require us to compute difficult high-dimensional integrals that are found,
for example, in the denominator of Bayesian posterior distributions. 

Like Gibbs sampling, the Metropolis algorithm is a MCMC sampling method, and generates a sequence of random
variables forming a Markov Chain whose invariant distribution is equal to the distribution from which we wish
to sample. Suppose that $h : \mathbb{R}^n \rightarrow \mathbb{R}$ is the probability density function of distribution, 
and suppose that $f(\theta) = ch(\theta)$ for some nonzero constant $c$ (in practice, we assume that $f$ is an easy
function to evaluate, while $h$ is difficult). Let $Q : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ be
a symmetric \emph{proposal function} 
(so that $Q(\cdot, y)$ is a probability density function for all $y \in \mathbb{R}^n$
 and $Q(x,y) = Q(y,x)$ for all $x,y \in \mathbb{R}^n$) and let 
 $A : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ be an \emph{acceptance function} defined by
\[
A(x,y) = \min\left(1, \frac{f(x)}{f(y)}\right).
\]
We combine these functions to sample from the aforementioned Markov Chain by following Algorithm \ref{alg:metropolis}.
\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Metropolis Algorithm}{}
    \State \textrm{Choose initial point } $x_0$.
    \For{$t=1,2,\ldots$}
        \State \textrm{Draw } $x' \sim Q(\cdot, x_{t-1})$
        \State \textrm{Draw } $a \sim \text{unif}(0,1)$
        \If{$a \leq A(x',x_{t-1})$}
            \State $x_t = x'$
        \Else
            \State $x_t = x_{t-1}$
        \EndIf
    \EndFor
    \State \textrm{Return } $x_1,x_2,x_3,\ldots$
\EndProcedure
\end{algorithmic}
\caption{Metropolis Algorithm}
\label{alg:metropolis}
\end{algorithm}
The Metropolis algorithm can be interpreted as follows:
given our current state $y$, we propose a new state according to the distribution $Q(\cdot, y)$. We then accept or reject it according to $A$, and continue the process. So long as $Q$ defines an irreducible, non-null recurrent, and aperiodic Markov chain, we will have a Markov chain whose unique invariant distribution will have density $h$. Furthermore, given any initial state, the chain will converge to this invariant distribution.

We will use the Metropolis algorithm to obtain samples from a multivariate normal distribution to demonstrate this process.
Suppose also that we desire to obtain samples from a multivariate normal distribution with arbitrary covariance matrix $\Sigma$, and that this is difficult (obviously we can do this directly in Python, but this is merely a tutorial to see how the Metropolis algorithm works). Suppose further that we are able to easily compute the ratio of the density of this distribution at two points $\mathbf{x}$ and $\mathbf{y}$ of length $K$, i.e.
\begin{align*}
\frac{N(\mathbf{x} \; ; \; \mu, \Sigma)}{N(\mathbf{y} \; ; \; \mu, \Sigma)} & = \frac{\frac{1}{(2\pi)^{K/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\mathbf{x} - \mu)^{T} \Sigma^{-1} (\mathbf{x} - \mu)}}{\frac{1}{(2\pi)^{K/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\mathbf{y} - \mu)^{T} \Sigma^{-1} (\mathbf{x} - \mu)}} \\
& = \frac{e^{-\frac{1}{2}(\mathbf{x} - \mu)^{T} \Sigma^{-1} (\mathbf{x} - \mu)}}{e^{-\frac{1}{2}(\mathbf{y} - \mu)^{T} \Sigma^{-1} (\mathbf{x} - \mu)}} \\
& = e^{-\frac{1}{2}\left((\mathbf{x} - \mu)^{T} \Sigma^{-1} (\mathbf{x} - \mu) - (\mathbf{y} - \mu)^{T} \Sigma^{-1} (\mathbf{y} - \mu)\right)}
\end{align*}

\begin{problem} \label{problem1}
Write an acceptance function that computes
\begin{equation*}
p = \min \{1, e^{-\frac{1}{2}\left((\mathbf{x} - \mu)^{T} \Sigma^{-1} (\mathbf{x} - \mu) - (\mathbf{y} - \mu)^{T} \Sigma^{-1} (\mathbf{y} - \mu)\right)}\}
\end{equation*}
given $\mathbf{x}, \mathbf{y}, \mu,$ and $\Sigma$, and then draws from a Bernoulli distribution with parameter $p$. It should return a $1$ if it accepts the new state, and a $0$ if it rejects it.
\end{problem}

Specifically, we will try to sample from the distribution centered at the origin, with covariance matrix
\begin{equation*}
\Sigma = \left[ \begin{array}{cc} 12 & 4 \\ 4 & 16 \end{array} \right]
\end{equation*}

\begin{lstlisting}
>>> mu = np.zeros(2)
>>> sigma = np.array([[12., 10.], [10., 16.]])
\end{lstlisting}

We will let $Q(\mathbf{x} | \mathbf{y}) = N(\mathbf{x} \; ; \; \mathbf{y}, I)$ be our proposal distribution, given that we are currently in state $\mathbf{y}$, i.e. we propose a new state by drawing from the multivariate normal distribution centered at $\mathbf{y}$ with identity covariance. We then accept according to our acceptance probability, computed in Problem \ref{problem1}.

\begin{problem}
Write a function that accepts a current state, the mean and covariance from the distribution we desire to sample from, and returns the next state. We should propose according to $Q$ described above, and accept according to the function in Problem \ref{problem1}.
\end{problem}

We now have a way to sample a new state from an old state. As we've stated before, this method creates a Markov chain that \emph{converges} to the desired distribution; at the beginning, however, if our initial guess is highly unlikely for the desired distribution, it may take a while before we get there. We would like to measure our progress.

\begin{problem}
Write a function that computes the log of the multivariate normal density of a point $\mathbf{x}$ given a mean $\mu$ and covariance matrix $\Sigma$. Be intelligent about how you implement this, that is, do not simply compute the multivariate normal density and then take the log of it, as this may lead to numerical issues. The whole purpose of looking at the multivariate log is to make this more stable.
\end{problem}

We will finally put everything together.

\begin{problem}
Write a function that accepts an initial point $\mathbf{x}$, a mean $\mu$ and covariance $\Sigma$ for the desired sampling distribution, and which performs the Metropolis algorithm for a number of iterations, $n\_samples$. Save each sample $\mathbf{x}$ as produced by the algorithm. Also compute the log of the multivariate normal density of each point, and return both the samples and the logprobs.
\end{problem}

We would like to see how long it takes for our algorithm to converge to the right distribution. We can do this by plotting the log-probs returned by our function. Here we use an initial state $\mathbf{x} = \left[\begin{array}{cc} 100 & 100 \end{array}\right]$.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{logprobs.pdf}
\caption{Log probabilities of our samples.}
\end{figure}

From this we can see that after between $300$ and $500$ iterations, we had converged to the correct distribution. We can visualize the path of our sampler by plotting the samples themselves:
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{samples.pdf}
\caption{Samples from the Metropolis algorithm.}
\end{figure}

\begin{problem}
Using $\mu$ and $\Sigma$ as defined previously and using an initial state $\mathbf{x} = \left[ \begin{array}{cc} 1000 & -1000 \end{array} \right]$ run your Metropolis sampler for $10000$ iterations. Plot the log probs as well as the samples. How long did it take to converge?
\end{problem}

In statistical mechanics, the Ising model describes how atoms interact in ferromagnetic material. More specifically, we assume that there is some lattice $\Lambda$ of sites. We say $i \sim j$ if $i$ and $j$ are adjacent sites. With each site $i$ in our lattice is an associated \emph{spin} $\sigma_{i} \in \{\pm 1\}$. A \emph{state} in our Ising model is a particular spin configuration $\sigma = (\sigma_{j})_{j \in \Lambda}$. If $L = |\Lambda|$, then there are $2^{L}$ possible states in our model. If $L$ is large, the state space becomes huge, which is why MCMC sampling methods (in particular the Metropolis algorithm) are so useful in calculating model estimations.

With any spin configuration $\sigma$, there is an associated energy $H(\sigma) = -J \sum_{i \sim j} \sigma_{i} \sigma_{j}$ where $J > 0$ for ferromagnetic materials, and $J < 0$ for antiferromagnetic materials. Throughout this lab, we will assume $J = 1$, leaving the energy equation to be $H(\sigma) = -\sum_{i \sim j} \sigma_{i}\sigma_{j}$ where the interaction from each pair is added only once.

We will consider a lattice that is a $100 \times 100$ square grid. The adjacent sites for a given site are those directly above, below, to the left, and right of the site, so to speak. For sites on the edge of the grid, we assume it wraps around. In other words, a site at the furthest left of the grid is adjacent to the corresponding site on the furthest right. Thus, a single spin configuration can be represented as a $100 \times 100$ array, with entries from $\{\pm 1\}$.

\begin{problem}
Write a function that initializes a spin configuration for an $n \times n$ lattice. It should return an $n \times n$ array, each entry of which is either $1$ or $-1$, chosen randomly. Test this for the grid described above, and plot the spin configuration using \li{matplotlib.pyplot.imshow}. It should look fairly random, as below.
\end{problem}

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{init.pdf}
\caption{Spin configuration from random initialization.}
\end{figure}

\begin{problem}
Write a function that computes the energy of a spin configuration with an $n \times n$ lattice described above. Make sure that you do not double count site pair interactions!
\end{problem}

Different spin configurations occur with different probabilities, depending on the energy of the spin configuration and $\beta > 0$, an inverse function of temperature. More specifically, for a given $\beta$, we have
\begin{equation*}
\mathbb{P}_{\beta}(\sigma) = \frac{e^{-\beta H(\sigma)}}{Z_{\beta}}
\end{equation*}
where $Z_{\beta} = \sum_{\sigma} e^{-\beta H(\sigma)}$. Because for our particular lattice there are $2^{100 \cdot 100} = 2^{10000}$ possible spin configurations, computing this sum is unfeasible. However, the numerator is quite simple, provided we can efficiently compute the energy $H(\sigma)$ of a spin configuration. Thus the ratio of the probability densities of two spin configurations is simple:
\begin{align*}
\frac{\mathbb{P}_{\beta}(\sigma^{*})}{\mathbb{P}_{\beta}(\sigma)} & = \frac{e^{-\beta H(\sigma^{*})}}{e^{-\beta H(\sigma)}} \\
& = e^{\beta (H(\sigma) - H(\sigma^{*}))}
\end{align*}

The simplicity of this ratio should lead us to think that a Metropolis algorithm might be appropriate by which to sample from the spin configuration probability distribution, in which case our acceptance probability would be
\begin{equation*}
A(\sigma^{*} | \sigma) = \begin{cases} 1 & \mbox{if } H(\sigma^{*}) < H(\sigma) \\ e^{\beta (H(\sigma) - H(\sigma^{*}))} & \mbox{ otherwise.} \end{cases}
\end{equation*}

By choosing our transition matrix $Q$ cleverly, we can also make it easy to compute the energy for any proposed spin configuration. We restrict our possible proposals to only those spin configurations in which we have flipped the spin at exactly one lattice site, i.e. we choose a lattice site $i$ and flip its spin. Thus, there are only $L$ possible proposal spin configurations $\sigma^{*}$ given $\sigma$, each being proposed with probability $\frac{1}{L}$, and such that $\sigma_{j}^{*} = \sigma_{j}$ for all $j \neq i$, and $\sigma_{i}^{*} = - \sigma_{i}$. Note that we would never actually write out this matrix (it would be $2^{10000} \times 2^{10000}$!!!). Computing the proposed sites energy is simple: if the spin flip site is $i$, then we have $H(\sigma^{*}) = H(\sigma) + 2\sum_{j: j \sim i} \sigma_{i}\sigma_{j}$.

\begin{problem}
Write a function that proposes a new spin configuration given the current spin configuration on an $n \times n$ lattice described above. This function simply needs to return a pair of indices $i,j$, chosen with probability $\frac{1}{n^{2}}$.
\end{problem}

\begin{problem}
Write a function that computes the energy of a proposed spin configuration, given the current spin configuration, its energy, and the proposed spin flip site indices.
\end{problem}

\begin{problem}
Write a function that accepts or rejects a proposed spin configuration, given the current configuration. It should accept the current energy, proposed energy, and $\beta$, and return a boolean.
\end{problem}

As in the last lab, we would like to look at the probabilities of each sample at each time. However, this would require us to compute the denominator $Z_{\beta}$, which as we explained previously is generally the reason we have to use a Metropolis algorithm to begin with. However, $Z_{\beta}$ is independent of any particular spin configuration, so we can just keep track of the numerator in the probability density function, which is $e^{-\beta H(\sigma)}$. In fact, we can take the log of this, and examine only $-\beta H(\sigma)$. We should see this increase as the algorithm proceeds, and converge once we are sampling from the correct distribution.

\begin{problem}
Write a function that initializes a spin configuration for an $n \times n$ lattice as done previously, and then performs the Metropolis algorithm, choosing new spin configurations and accepting or rejecting them. It should burn in first, and then iterate $n\_samples$ times, keeping every $100^{\text{th}}$ sample (this is to prevent memory failure) and all of the above values $-\beta H(\sigma)$ (keep the values even for the burn in period). It should also accept $\beta$ as an argument, allowing us to effectively adjust the temperature for the model.
\end{problem}

\begin{problem}
Test your Metropolis sampler on a $100 \times 100$ grid, with $200000$ iterations, with $n\_samples$ large enough to get $50$ samples, testing with $\beta = 1$ and then with $\beta = 0.2$. Plot the proportional log probabilities, and also plot a late sample from each test using \li{matplotlib.pyplot.imshow}. How does the ferromagnetic material behave differently with differing temperatures? Recall that $\beta$ is an inverse function of temperature. You should see more structure with lower temperature, as shown in the sample.
\end{problem}

\begin{figure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{beta1_logprobs.pdf}
		\caption{Proportional log probs when $\beta = 1$.}
		\label{figur:1}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\includegraphics[width=.75\textwidth]{beta1.pdf}
		\caption{Spin configuration sample when $\beta = 1$.}
		\label{figur:2}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{beta0_2_logprobs.pdf}
		\caption{Proportional log probs when $\beta = 0.2$.}
		\label{figur:3}
	\end{subfigure}
	\begin{subfigure}[b]{.49\textwidth}
		\centering
		\includegraphics[width=.75\textwidth]{beta0_2.pdf}
		\caption{Spin configuration sample when $\beta = 0.2$.}
		\label{figur:4}
	\end{subfigure}
\end{figure}
